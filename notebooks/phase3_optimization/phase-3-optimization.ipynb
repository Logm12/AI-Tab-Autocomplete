{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13975549,"sourceType":"datasetVersion","datasetId":8859257},{"sourceId":14057036,"sourceType":"datasetVersion","datasetId":8947251},{"sourceId":14192114,"sourceType":"datasetVersion","datasetId":9049456}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phase 3: Optimization (CPU Version)\n\nThis notebook performs the following steps:\n1.  **Setup**: Install dependencies and build `llama.cpp`.\n2.  **Merge**: Merge the LoRA adapter (from `model.zip`) with the Base Model (`Qwen2.5-Coder-0.5B-Instruct`).\n3.  **Convert**: Convert the merged model to GGUF format.\n4.  **Quantize**: Quantize the model to 4-bit (`q4_k_m`).\n5.  **Benchmark**: Test performance using `llama-bench`.\n\n**Hardware**: CPU (GPU is not required for this notebook).","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup Environment","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n!pip uninstall -y -q onnx tensorflow-metadata bigframes opentelemetry-proto\n\n!pip install -q -U transformers huggingface_hub sentencepiece\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q protobuf==3.20.3\n\nprint(\"SETUP COMPLETE.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T08:18:40.717871Z","iopub.execute_input":"2025-12-08T08:18:40.718161Z","iopub.status.idle":"2025-12-08T08:20:57.884146Z","shell.execute_reply.started":"2025-12-08T08:18:40.718130Z","shell.execute_reply":"2025-12-08T08:20:57.882616Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-datasets 4.9.9 requires tensorflow-metadata, which is not installed.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSETUP COMPLETE.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Clone and build llama.cpp using CMake\n!git clone https://github.com/ggerganov/llama.cpp\n%cd llama.cpp\n!cmake -B build\n!cmake --build build --config Release -j$(nproc)\n%cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:37:35.714284Z","iopub.execute_input":"2025-12-17T09:37:35.714632Z","iopub.status.idle":"2025-12-17T09:44:53.675060Z","shell.execute_reply.started":"2025-12-17T09:37:35.714607Z","shell.execute_reply":"2025-12-17T09:44:53.674020Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 72473, done.\u001b[K\nremote: Counting objects: 100% (399/399), done.\u001b[K\nremote: Compressing objects: 100% (307/307), done.\u001b[K\nremote: Total 72473 (delta 263), reused 92 (delta 92), pack-reused 72074 (from 3)\u001b[K\nReceiving objects: 100% (72473/72473), 242.47 MiB | 33.18 MiB/s, done.\nResolving deltas: 100% (52308/52308), done.\n/kaggle/working/llama.cpp\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/cc\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- ggml version: 0.9.4\n-- ggml commit:  487674fbb\n-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n-- Configuring done (2.3s)\n-- Generating done (0.4s)\n-- Build files have been written to: /kaggle/working/llama.cpp/build\n[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n[  0%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n[  0%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n[  0%] Built target build_info\n[  1%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n[  1%] Built target sha256\n[  1%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n[  1%] Built target sha1\n[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n[  1%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n[  2%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n[  2%] Built target xxhash\n[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n[  2%] Built target llama-llava-cli\n[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n[  3%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n[  3%] Built target llama-gemma3-cli\n[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n[  3%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n[  3%] Built target llama-minicpmv-cli\n[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n[  4%] Built target llama-qwen2vl-cli\n[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n[  5%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n[  5%] Built target ggml-base\n[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n[  9%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n[  9%] Built target cpp-httplib\n[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n[  9%] Built target ggml-cpu\n[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n[ 10%] Built target ggml\n[ 10%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n[ 10%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n[ 10%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n[ 10%] Built target llama-gguf\n[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n[ 12%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n[ 12%] Built target llama-gguf-hash\n[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n[ 42%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n[ 42%] Built target llama\n[ 42%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n[ 43%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n[ 43%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n[ 44%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n[ 44%] Built target test-c\n[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n[ 44%] Built target llama-simple\n[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n[ 44%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n[ 44%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n[ 44%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n[ 45%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n[ 46%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n[ 47%] Built target llama-simple-chat\n[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n[ 48%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n[ 50%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n[ 50%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n[ 50%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n[ 50%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n[ 51%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n[ 51%] Built target mtmd\n[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n[ 54%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n[ 54%] Built target common\n[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n[ 54%] Built target test-tokenizer-0\n[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n[ 56%] Built target test-sampling\n[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n[ 57%] Built target test-grammar-parser\n[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n[ 58%] Built target test-llama-grammar\n[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n[ 58%] Built target test-grammar-integration\n[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n[ 58%] Built target test-quantize-stats\n[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n[ 59%] Built target test-gbnf-validator\n[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n[ 59%] Built target test-tokenizer-1-bpe\n[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n[ 60%] Built target test-tokenizer-1-spm\n[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n[ 60%] Built target test-json-schema-to-grammar\n[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n[ 62%] Built target test-chat-template\n[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n[ 62%] Built target test-json-partial\n[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n[ 63%] Built target test-log\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n[ 63%] Built target test-chat-parser\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n[ 64%] Built target test-regex-partial\n[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n[ 66%] Built target test-thread-safety\n[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n[ 66%] Built target test-chat-peg-parser\n[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n[ 67%] Built target test-arg-parser\n[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n[ 68%] Built target test-opt\n[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n[ 70%] Built target test-gguf\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n[ 70%] Built target test-model-load-cancel\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n[ 71%] Built target test-autorelease\n[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n[ 72%] Built target test-state-restore-fragmented\n[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n[ 73%] Built target test-chat\n[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n[ 73%] Built target test-barrier\n[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n[ 74%] Built target test-peg-parser\n[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n[ 74%] Built target test-quantize-fns\n[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n[ 75%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n[ 75%] Built target test-mtmd-c-api\n[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n[ 77%] Built target test-rope\n[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n[ 78%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n[ 79%] Built target test-quantize-perf\n[ 80%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n[ 80%] Built target test-alloc\n[ 80%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n[ 80%] Built target llama-batched\n[ 80%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n[ 80%] Built target llama-eval-callback\n[ 80%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n[ 80%] Built target llama-embedding\n[ 80%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n[ 81%] Built target llama-idle\n[ 81%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n[ 82%] Built target llama-lookahead\n[ 82%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n[ 82%] Built target llama-lookup\n[ 82%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n[ 83%] Built target llama-lookup-create\n[ 83%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n[ 83%] Built target llama-lookup-merge\n[ 83%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n[ 84%] Built target llama-lookup-stats\n[ 84%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n[ 85%] Built target llama-parallel\n[ 85%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n[ 85%] Built target llama-passkey\n[ 85%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n[ 85%] Built target llama-retrieval\n[ 86%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n[ 86%] Built target llama-save-load-state\n[ 86%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n[ 87%] Built target llama-speculative\n[ 87%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n[ 87%] Built target llama-gen-docs\n[ 87%] Built target llama-speculative-simple\n[ 87%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n[ 87%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n[ 87%] Built target llama-logits\n[ 87%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n[ 87%] Built target llama-finetune\n[ 87%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n[ 87%] Built target llama-vdot\n[ 87%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n[ 87%] Built target llama-convert-llama2c-to-ggml\n[ 87%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n[ 87%] Built target llama-diffusion-cli\n[ 87%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n[ 88%] Built target llama-q8dot\n[ 88%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n[ 88%] Built target llama-gguf-split\n[ 89%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n[ 89%] Built target llama-batched-bench\n[ 89%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n[ 89%] Built target test-backend-ops\n[ 90%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n[ 90%] Built target llama-imatrix\n[ 90%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n[ 90%] Built target llama-completion\n[ 90%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n[ 90%] Built target llama-bench\n[ 91%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n[ 91%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n[ 91%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n[ 91%] Built target llama-quantize\n[ 91%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n[ 91%] Built target llama-perplexity\n[ 92%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n[ 92%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n[ 93%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n[ 93%] Built target llama-tokenize\n[ 93%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n[ 94%] Built target llama-mtmd-cli\n[ 95%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n[ 95%] Built target llama-run\n[ 96%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n[ 96%] Built target llama-cvector-generator\n[ 97%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n[ 97%] Built target llama-export-lora\n[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n[ 97%] Built target llama-fit-params\n[ 97%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n[ 97%] Built target server-context\n[ 97%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n[ 98%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n[ 98%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n[ 98%] Built target llama-tts\n[ 98%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n[ 98%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n[ 99%] Built target llama-cli\n[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-task.cpp.o\u001b[0m\n[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-queue.cpp.o\u001b[0m\n[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-common.cpp.o\u001b[0m\n[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-context.cpp.o\u001b[0m\n[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n[100%] Built target llama-server\n/kaggle/working\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Prepare Data\nUpload model to Kaggle","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\nimport shutil\nINPUT_PATH = \"/kaggle/input/ai-auto\"  # Kaggle dataset path\nSFT_ADAPTER_NAME = \"final_model\"           # SFT adapter folder name\nDPO_ADAPTER_NAME = \"dpo_final_model\"       # DPO adapter folder name\nSFT_ADAPTER_DIR = \"sft_adapter\"\nDPO_ADAPTER_DIR = \"dpo_adapter\" \nSFT_MERGED_DIR = \"sft_merged_model\"\nDPO_MERGED_DIR = \"dpo_merged_model\"\nBASE_MODEL_ID = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\ndef find_and_copy_adapter(input_path, adapter_name, output_dir):\n    if os.path.isdir(input_path):\n        for root, dirs, files in os.walk(input_path):\n            if adapter_name in dirs:\n                src = os.path.join(root, adapter_name)\n                if os.path.exists(os.path.join(src, \"adapter_config.json\")):\n                    if os.path.exists(output_dir):\n                        shutil.rmtree(output_dir)\n                    shutil.copytree(src, output_dir)\n                    print(f\"✓ Found {adapter_name} -> {output_dir}\")\n                    return True\n            if \"adapter_config.json\" in files and adapter_name in root:\n                if os.path.exists(output_dir):\n                    shutil.rmtree(output_dir)\n                shutil.copytree(root, output_dir)\n                print(f\"✓ Found {adapter_name} -> {output_dir}\")\n                return True\n    return False\nsft_found = find_and_copy_adapter(INPUT_PATH, SFT_ADAPTER_NAME, SFT_ADAPTER_DIR)\nif not sft_found:\n    print(f\"SFT adapter '{SFT_ADAPTER_NAME}' not found\")\ndpo_found = find_and_copy_adapter(INPUT_PATH, DPO_ADAPTER_NAME, DPO_ADAPTER_DIR)\nif not dpo_found:\n    print(f\"DPO adapter '{DPO_ADAPTER_NAME}' not found\")\nprint(f\"\\nSFT adapter: {'✓' if sft_found else '✗'}\")\nprint(f\"DPO adapter: {'✓' if dpo_found else '✗'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:47:32.772363Z","iopub.execute_input":"2025-12-17T09:47:32.772755Z","iopub.status.idle":"2025-12-17T09:47:34.111023Z","shell.execute_reply.started":"2025-12-17T09:47:32.772723Z","shell.execute_reply":"2025-12-17T09:47:34.110263Z"}},"outputs":[{"name":"stdout","text":"✓ Found final_model -> sft_adapter\n✓ Found dpo_final_model -> dpo_adapter\n\nSFT adapter: ✓\nDPO adapter: ✓\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Merge Model (Base + Adapter)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\nimport gc\ndef merge_lora_to_base(base_model_id, adapter_dir, output_dir, model_name=\"Model\"):\n    if not os.path.exists(adapter_dir):\n        print(f\"{model_name}: Adapter not found at {adapter_dir}\")\n        return False\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"Processing: {model_name}\")\n    print(f\"{'='*50}\")\n    \n    print(f\"Loading base model: {base_model_id}\")\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_id,\n        torch_dtype=torch.float16,\n        device_map=\"cpu\",\n        trust_remote_code=True\n    )\n    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n    \n    print(f\"Loading adapter from: {adapter_dir}\")\n    model = PeftModel.from_pretrained(base_model, adapter_dir)\n    \n    print(\"Merging...\")\n    model = model.merge_and_unload()\n    \n    print(f\"Saving to: {output_dir}\")\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    del model, base_model\n    gc.collect()\n    \n    print(f\"{model_name} merged successfully\")\n    return True\nif os.path.exists(SFT_ADAPTER_DIR):\n    merge_lora_to_base(BASE_MODEL_ID, SFT_ADAPTER_DIR, SFT_MERGED_DIR, \"SFT Model\")\nif os.path.exists(DPO_ADAPTER_DIR):\n    merge_lora_to_base(BASE_MODEL_ID, DPO_ADAPTER_DIR, DPO_MERGED_DIR, \"DPO Model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:47:47.468329Z","iopub.execute_input":"2025-12-17T09:47:47.469008Z","iopub.status.idle":"2025-12-17T09:48:47.786029Z","shell.execute_reply.started":"2025-12-17T09:47:47.468978Z","shell.execute_reply":"2025-12-17T09:48:47.785004Z"}},"outputs":[{"name":"stderr","text":"2025-12-17 09:48:03.883467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765964884.105595      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765964884.167690      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"\n==================================================\nProcessing: SFT Model\n==================================================\nLoading base model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"303a4093cdac48c48988201cbbda0082"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"768ef19a27ff4edcaffc5cb9b305249a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72b5e9ef2a184d94a9d5d3b0d7a13939"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ab9242eae44ff6895dd4784f1bdd16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7422b1e2bfd4ed290b4f9827acf11b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af55c40db0c4d24ae1caceac19f0b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b31da48895cc4633a8b9a647cf16a7df"}},"metadata":{}},{"name":"stdout","text":"Loading adapter from: sft_adapter\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['alora_invocation_tokens', 'arrow_config', 'ensure_weight_tying', 'peft_version', 'target_parameters'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Merging...\nSaving to: sft_merged_model\nSFT Model merged successfully\n\n==================================================\nProcessing: DPO Model\n==================================================\nLoading base model: Qwen/Qwen2.5-Coder-0.5B-Instruct\nLoading adapter from: dpo_adapter\nMerging...\nSaving to: dpo_merged_model\nDPO Model merged successfully\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 4. Convert to GGUF and Quantize","metadata":{}},{"cell_type":"code","source":"!pip install -r llama.cpp/requirements.txt\n\ndef convert_and_quantize(merged_dir, output_name, quant_type=\"q4_k_m\"):\n    if not os.path.exists(merged_dir):\n        print(f\"{merged_dir} not found, skipping...\")\n        return None\n    \n    f16_path = f\"{output_name}-f16.gguf\"\n    quantized_path = f\"{output_name}-{quant_type}.gguf\"\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"Converting: {merged_dir}\")\n    print(f\"{'='*50}\")\n    \n    !python llama.cpp/convert_hf_to_gguf.py {merged_dir} --outfile {f16_path} --outtype f16\n    \n    quantize_bin = \"llama.cpp/build/bin/llama-quantize\"\n    if not os.path.exists(quantize_bin):\n        quantize_bin = \"llama.cpp/llama-quantize\"\n    \n    !./{quantize_bin} {f16_path} {quantized_path} {quant_type}\n    \n    if os.path.exists(f16_path):\n        os.remove(f16_path)\n    \n    if os.path.exists(quantized_path):\n        size_mb = os.path.getsize(quantized_path) / (1024*1024)\n        print(f\"✓ Created: {quantized_path} ({size_mb:.1f} MB)\")\n        return quantized_path\n    return None\nsft_gguf = convert_and_quantize(SFT_MERGED_DIR, \"qwen2.5-coder-sft\")\ndpo_gguf = convert_and_quantize(DPO_MERGED_DIR, \"qwen2.5-coder-dpo\")\nprint(\"\\n\" + \"=\"*50)\nprint(\"SUMMARY\")\nprint(\"=\"*50)\nif sft_gguf:\n    print(f\"✓ SFT GGUF: {sft_gguf}\")\nif dpo_gguf:\n    print(f\"✓ DPO GGUF: {dpo_gguf}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T08:33:19.920123Z","iopub.execute_input":"2025-12-08T08:33:19.921017Z","iopub.status.idle":"2025-12-08T08:34:09.699886Z","shell.execute_reply.started":"2025-12-08T08:33:19.920979Z","shell.execute_reply":"2025-12-08T08:34:09.698426Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\nIgnoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\nIgnoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\nRequirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\nRequirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.1)\nRequirement already satisfied: transformers<5.0.0,>=4.57.1 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.57.3)\nCollecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 6))\n  Downloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 7))\n  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nRequirement already satisfied: torch~=2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2.6.0+cu124)\nCollecting aiohttp~=3.9.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 1))\n  Downloading https://download.pytorch.org/whl/nightly/aiohttp-3.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pytest~=8.3.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 2))\n  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: huggingface_hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 3)) (0.36.0)\nCollecting matplotlib~=3.10.0 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 4))\n  Downloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nCollecting openai~=1.55.3 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 6))\n  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: pandas~=2.2.3 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 7)) (2.2.3)\nCollecting prometheus-client~=0.20.0 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 8))\n  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2.32.5)\nCollecting wget~=3.2 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 10))\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting typer~=0.15.1 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 11))\n  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\nCollecting seaborn~=0.13.2 (from -r llama.cpp/./requirements/requirements-tool_bench.txt (line 12))\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (3.20.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.57.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (6.7.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (1.22.0)\nRequirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 2)) (2.1.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest~=8.3.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 2)) (1.6.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.34.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 3)) (1.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.4.8)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (11.3.0)\nRequirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (4.11.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.10.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (2.12.4)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 9)) (2025.10.5)\nCollecting click<8.2,>=8.0.0 (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11))\n  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (14.2.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 6)) (0.4.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 1)) (0.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch~=2.6.0->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy~=1.26.4->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r llama.cpp/./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\nDownloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typer-0.15.4-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=c9490e544063093bb4a3aa8d36f430d27c0c76e91b878276ab57f17c16d886b2\n  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\nSuccessfully built wget\nInstalling collected packages: wget, pytest, protobuf, prometheus-client, click, aiohttp, typer, openai, matplotlib, seaborn, gguf\n  Attempting uninstall: pytest\n    Found existing installation: pytest 8.4.1\n    Uninstalling pytest-8.4.1:\n      Successfully uninstalled pytest-8.4.1\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: prometheus-client\n    Found existing installation: prometheus_client 0.22.1\n    Uninstalling prometheus_client-0.22.1:\n      Successfully uninstalled prometheus_client-0.22.1\n  Attempting uninstall: click\n    Found existing installation: click 8.3.0\n    Uninstalling click-8.3.0:\n      Successfully uninstalled click-8.3.0\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.13.2\n    Uninstalling aiohttp-3.13.2:\n      Successfully uninstalled aiohttp-3.13.2\n  Attempting uninstall: typer\n    Found existing installation: typer 0.16.0\n    Uninstalling typer-0.16.0:\n      Successfully uninstalled typer-0.16.0\n  Attempting uninstall: openai\n    Found existing installation: openai 2.7.1\n    Uninstalling openai-2.7.1:\n      Successfully uninstalled openai-2.7.1\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.2\n    Uninstalling matplotlib-3.7.2:\n      Successfully uninstalled matplotlib-3.7.2\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.12.2\n    Uninstalling seaborn-0.12.2:\n      Successfully uninstalled seaborn-0.12.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-datasets 4.9.9 requires tensorflow-metadata, which is not installed.\nydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.10.0 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\nlitellm 1.76.3 requires aiohttp>=3.10, but you have aiohttp 3.9.5 which is incompatible.\nlitellm 1.76.3 requires openai>=1.99.5, but you have openai 1.55.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohttp-3.9.5 click-8.1.8 gguf-0.17.1 matplotlib-3.10.7 openai-1.55.3 prometheus-client-0.20.0 protobuf-4.25.8 pytest-8.3.5 seaborn-0.13.2 typer-0.15.4 wget-3.2\nINFO:hf-to-gguf:Loading model: merged_model\nINFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\nINFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {896, 151936}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.0.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.0.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.0.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.1.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.1.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.1.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.10.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.10.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.10.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.11.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.11.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.11.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.12.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.12.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.12.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.13.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.13.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.13.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.14.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.14.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.14.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.15.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.15.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.15.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.16.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.16.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.16.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.17.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.17.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.17.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.18.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.18.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.18.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.19.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.19.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.19.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.2.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.2.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.2.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.20.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.20.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.20.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.21.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.21.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.21.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.22.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.22.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.22.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.23.attn_k.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.23.attn_q.bias,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.23.attn_v.bias,        torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.3.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.3.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.3.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.4.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.4.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.4.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.5.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.5.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.5.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.6.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.6.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.6.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.7.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.7.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.7.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.8.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.8.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.8.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {4864, 896}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {896, 4864}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.9.attn_k.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.9.attn_q.bias,         torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {896, 896}\nINFO:hf-to-gguf:blk.9.attn_v.bias,         torch.float16 --> F32, shape = {128}\nINFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {896, 128}\nINFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {896}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 32768\nINFO:hf-to-gguf:gguf: embedding length = 896\nINFO:hf-to-gguf:gguf: feed forward length = 4864\nINFO:hf-to-gguf:gguf: head count = 14\nINFO:hf-to-gguf:gguf: key-value head count = 2\nINFO:hf-to-gguf:gguf: rope theta = 1000000.0\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nThe tokenizer you are loading from 'merged_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\nINFO:gguf.vocab:Adding 151387 merge(s).\nINFO:gguf.vocab:Setting special token type eos to 151645\nINFO:gguf.vocab:Setting special token type pad to 151643\nINFO:gguf.vocab:Setting special token type bos to 151643\nINFO:gguf.vocab:Setting add_bos_token to False\nINFO:gguf.vocab:Setting chat_template to {%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:qwen2.5-coder-0.5b-merged-f16.gguf: n_tensors = 290, total_size = 988.2M\nWriting: 100%|█████████████████████████████| 988M/988M [00:02<00:00, 491Mbyte/s]\nINFO:hf-to-gguf:Model successfully exported to qwen2.5-coder-0.5b-merged-f16.gguf\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 6. Benchmarking","metadata":{}},{"cell_type":"code","source":"# Run benchmark\nbench_bin = \"llama.cpp/build/bin/llama-bench\"\ncli_bin = \"llama.cpp/build/bin/llama-cli\"\n\n# Check and build if missing\nif not os.path.exists(bench_bin):\n    if os.path.exists(\"llama.cpp/llama-bench\"): bench_bin = \"llama.cpp/llama-bench\"\nif not os.path.exists(cli_bin):\n    if os.path.exists(\"llama.cpp/llama-cli\"): cli_bin = \"llama.cpp/llama-cli\"\n    elif os.path.exists(\"llama.cpp/main\"): cli_bin = \"llama.cpp/main\"\n\nprint(\"=== THROUGHPUT BENCHMARK ===\")\nif os.path.exists(bench_bin):\n    !./{bench_bin} -m {quantized_gguf} -n 128 -p 1024\nelse:\n    print(f\"ERROR: Could not find llama-bench at {bench_bin}\")\n\nprint(\"\\n=== LATENCY TEST (Single Request) ===\")\nif os.path.exists(cli_bin):\n    !./{cli_bin} -m {quantized_gguf} -p \"def fibonacci(n):\" -n 32 -c 1024 -t 2\nelse:\n    print(f\"ERROR: Could not find llama-cli at {cli_bin}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T08:41:10.944925Z","iopub.execute_input":"2025-12-08T08:41:10.946475Z","iopub.status.idle":"2025-12-08T08:44:12.465090Z","shell.execute_reply.started":"2025-12-08T08:41:10.946404Z","shell.execute_reply":"2025-12-08T08:44:12.463497Z"}},"outputs":[{"name":"stdout","text":"=== THROUGHPUT BENCHMARK ===\n| model                          |       size |     params | backend    | threads |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\n| qwen2 1B Q4_K - Medium         | 373.71 MiB |   494.03 M | CPU        |       2 |          pp1024 |         62.41 ± 0.08 |\n| qwen2 1B Q4_K - Medium         | 373.71 MiB |   494.03 M | CPU        |       2 |           tg128 |         23.17 ± 0.09 |\n\nbuild: 4d3726278 (7315)\n\n=== LATENCY TEST (Single Request) ===\nbuild: 7315 (4d3726278) with GNU 11.4.0 for Linux x86_64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 27 key-value pairs and 290 tensors from qwen2.5-coder-0.5b-merged-q4_k_m.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\nllama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.800000\nllama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.700000\nllama_model_loader: - kv   5:                               general.name str              = Merged_Model\nllama_model_loader: - kv   6:                         general.size_label str              = 494M\nllama_model_loader: - kv   7:                          qwen2.block_count u32              = 24\nllama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\nllama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 896\nllama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 4864\nllama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 14\nllama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\nllama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\nllama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  25:               general.quantization_version u32              = 2\nllama_model_loader: - kv  26:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  121 tensors\nllama_model_loader: - type q5_0:  132 tensors\nllama_model_loader: - type q8_0:   13 tensors\nllama_model_loader: - type q4_K:   12 tensors\nllama_model_loader: - type q6_K:   12 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 373.71 MiB (6.35 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 22\nload: token to piece cache size = 0.9310 MB\nprint_info: arch             = qwen2\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 896\nprint_info: n_embd_inp       = 896\nprint_info: n_layer          = 24\nprint_info: n_head           = 14\nprint_info: n_head_kv        = 2\nprint_info: n_rot            = 64\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 64\nprint_info: n_embd_head_v    = 64\nprint_info: n_gqa            = 7\nprint_info: n_embd_k_gqa     = 128\nprint_info: n_embd_v_gqa     = 128\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 4864\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: n_expert_groups  = 0\nprint_info: n_group_used     = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 1B\nprint_info: model params     = 494.03 M\nprint_info: general.name     = Merged_Model\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =   373.71 MiB\nload_tensors:   CPU_REPACK model buffer size =    28.05 MiB\n.................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 1024\nllama_context: n_ctx_seq     = 1024\nllama_context: n_batch       = 1024\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n\u001b[0mllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =    12.00 MiB\nllama_kv_cache: size =   12.00 MiB (  1024 cells,  24 layers,  1/1 seqs), K (f16):    6.00 MiB, V (f16):    6.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   298.50 MiB\nllama_context: graph nodes  = 823\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 1024\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n\u001b[0mmain: llama threadpool init, n_threads = 2\nmain: chat template is available, enabling conversation mode (disable it with -no-cnv)\n*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\n\u001b[0mmain: chat template example:\n<|im_start|>system\nYou are a helpful assistant<|im_end|>\n<|im_start|>user\nHello<|im_end|>\n<|im_start|>assistant\nHi there<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\n\n\nsystem_info: n_threads = 2 (n_threads_batch = 2) / 4 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 3253037083\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 1024\n\ttop_k = 20, top_p = 0.800, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n\u001b[33msampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 1024, n_batch = 2048, n_predict = 32, n_keep = 0\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\n*****************************\n\u001b[0m\u001b[0mIMPORTANT: The current llama-cli will be moved to llama-completion in the near future\n\u001b[0m  New llama-cli will have enhanced features and improved user experience\n\u001b[0m  More info: https://github.com/ggml-org/llama.cpp/discussions/17618\n\u001b[0m*****************************\n\u001b[0muser\ndef fibonacci(n):\nassistant\nHere is a Python function that calculates the nth Fibonacci number:\n\n```python\ndef fibonacci(n):\n    if n <= 0:\n        raise ValueError(\"Input must\u001b[1m\u001b[32m\n> \u001b[0m\ncommon_perf_print:    sampling time =      20.99 ms\ncommon_perf_print:    samplers time =       6.70 ms /    44 tokens\ncommon_perf_print:        load time =     586.37 ms\ncommon_perf_print: prompt eval time =     183.87 ms /    12 tokens (   15.32 ms per token,    65.26 tokens per second)\ncommon_perf_print:        eval time =    1298.46 ms /    31 runs   (   41.89 ms per token,    23.87 tokens per second)\ncommon_perf_print:       total time =   53330.09 ms /    43 tokens\ncommon_perf_print: unaccounted time =   51826.76 ms /  97.2 %      (total - sampling - prompt eval - eval) / (total)\ncommon_perf_print:    graphs reused =         31\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                  684 =   373 +      12 +     298                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                   28 =    28 +       0 +       0                |\nInterrupted by user\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from IPython.display import FileLink\n\nprint(\"Click to download:\")\ndisplay(FileLink(output_gguf))\ndisplay(FileLink(quantized_gguf))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T08:44:12.480776Z","iopub.execute_input":"2025-12-08T08:44:12.481122Z","iopub.status.idle":"2025-12-08T08:44:12.505006Z","shell.execute_reply.started":"2025-12-08T08:44:12.481096Z","shell.execute_reply":"2025-12-08T08:44:12.503901Z"}},"outputs":[{"name":"stdout","text":"Click to download:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/qwen2.5-coder-0.5b-merged-f16.gguf","text/html":"<a href='qwen2.5-coder-0.5b-merged-f16.gguf' target='_blank'>qwen2.5-coder-0.5b-merged-f16.gguf</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/qwen2.5-coder-0.5b-merged-q4_k_m.gguf","text/html":"<a href='qwen2.5-coder-0.5b-merged-q4_k_m.gguf' target='_blank'>qwen2.5-coder-0.5b-merged-q4_k_m.gguf</a><br>"},"metadata":{}}],"execution_count":9}]}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DPO Training for Code Autocomplete\n",
                "\n",
                "Train a model using Direct Preference Optimization (DPO) with preference pairs generated from self-play."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import sys\n",
                "\n",
                "def install_package(package):\n",
                "    try:\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
                "        print(f\"[install_package] Successfully installed {package}\")\n",
                "        return True\n",
                "    except Exception as e:\n",
                "        print(f\"[install_package] Failed to install {package}: {e}\")\n",
                "        return False\n",
                "\n",
                "print(\"========== START: Installing Packages ==========\")\n",
                "install_package(\"unsloth\")\n",
                "install_package(\"trl>=0.7.0\")\n",
                "install_package(\"transformers>=4.36.0\")\n",
                "install_package(\"datasets\")\n",
                "install_package(\"accelerate\")\n",
                "install_package(\"bitsandbytes\")\n",
                "install_package(\"peft\")\n",
                "print(\"========== END: Installing Packages ==========\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import torch\n",
                "from pathlib import Path\n",
                "\n",
                "print(\"========== START: Checking Environment ==========\")\n",
                "try:\n",
                "    from unsloth import FastLanguageModel\n",
                "    UNSLOTH_AVAILABLE = True\n",
                "    print(\"[Environment] Unsloth is available\")\n",
                "except ImportError:\n",
                "    print(\"[Environment] Unsloth not available\")\n",
                "    UNSLOTH_AVAILABLE = False\n",
                "\n",
                "try:\n",
                "    from trl import DPOTrainer, DPOConfig\n",
                "except ImportError:\n",
                "    try:\n",
                "        from trl import DPOTrainer\n",
                "        from transformers import TrainingArguments as DPOConfig\n",
                "        print(\"[Environment] Using TrainingArguments as DPOConfig fallback\")\n",
                "    except ImportError as e:\n",
                "        raise ImportError(f\"TRL library required: {e}\")\n",
                "\n",
                "try:\n",
                "    from datasets import Dataset\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"datasets library required: {e}\")\n",
                "\n",
                "try:\n",
                "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"transformers required: {e}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"[Environment] GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"[Environment] Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"WARNING: No GPU. Training will be very slow.\")\n",
                "print(\"========== END: Checking Environment ==========\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SFT_MODEL_PATH = \"./final_model\"\n",
                "DPO_DATA_PATH = \"dpo_preference_data.jsonl\"\n",
                "OUTPUT_DIR = \"./dpo_outputs\"\n",
                "FINAL_MODEL_DIR = \"./dpo_final_model\"\n",
                "\n",
                "MAX_SEQ_LENGTH = 2048\n",
                "LEARNING_RATE = 5e-5\n",
                "BATCH_SIZE = 2\n",
                "GRADIENT_ACCUMULATION = 4\n",
                "NUM_EPOCHS = 1\n",
                "BETA = 0.1\n",
                "MAX_PROMPT_LENGTH = 512\n",
                "MAX_LENGTH = 1024\n",
                "\n",
                "if not os.path.exists(SFT_MODEL_PATH):\n",
                "    print(f\"[Config] SFT model not found at {SFT_MODEL_PATH}\")\n",
                "    SFT_MODEL_PATH = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
                "    print(f\"[Config] Using base model: {SFT_MODEL_PATH}\")\n",
                "\n",
                "if not os.path.exists(DPO_DATA_PATH):\n",
                "    alt_paths = [\n",
                "        \"/content/dpo_preference_data.jsonl\",\n",
                "        \"./dpo_data_backup.jsonl\",\n",
                "        \"/content/dpo_data_backup.jsonl\"\n",
                "    ]\n",
                "    for p in alt_paths:\n",
                "        if os.path.exists(p):\n",
                "            DPO_DATA_PATH = p\n",
                "            print(f\"[Config] Found DPO data at: {DPO_DATA_PATH}\")\n",
                "            break\n",
                "    else:\n",
                "        print(\"WARNING: DPO data not found. Run 04_generate_dpo_data.ipynb first.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dpo_data(data_path):\n",
                "    print(f\"========== START: load_dpo_data ==========\")\n",
                "    print(f\"[load_dpo_data] Input: data_path={data_path}\")\n",
                "    data = []\n",
                "    \n",
                "    try:\n",
                "        with open(data_path, 'r', encoding='utf-8') as f:\n",
                "            for line_num, line in enumerate(f, 1):\n",
                "                try:\n",
                "                    item = json.loads(line.strip())\n",
                "                    if 'prompt' in item and 'chosen' in item and 'rejected' in item:\n",
                "                        data.append({\n",
                "                            'prompt': str(item['prompt']),\n",
                "                            'chosen': str(item['chosen']),\n",
                "                            'rejected': str(item['rejected']),\n",
                "                        })\n",
                "                except json.JSONDecodeError as e:\n",
                "                    if line_num <= 5:\n",
                "                        print(f\"[load_dpo_data] JSON error line {line_num}: {e}\")\n",
                "                    continue\n",
                "    except FileNotFoundError:\n",
                "        print(f\"[load_dpo_data] File not found: {data_path}\")\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        print(f\"[load_dpo_data] Error loading data: {e}\")\n",
                "        return None\n",
                "    \n",
                "    if len(data) == 0:\n",
                "        print(\"[load_dpo_data] No valid data loaded\")\n",
                "        return None\n",
                "    \n",
                "    print(f\"[load_dpo_data] Output: Loaded {len(data)} preference pairs\")\n",
                "    print(f\"========== END: load_dpo_data ==========\")\n",
                "    return Dataset.from_list(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_model_for_dpo(model_path):\n",
                "    print(f\"========== START: load_model_for_dpo ==========\")\n",
                "    print(f\"[load_model_for_dpo] Input: model_path={model_path}\")\n",
                "    \n",
                "    if UNSLOTH_AVAILABLE:\n",
                "        try:\n",
                "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "                model_name=model_path,\n",
                "                max_seq_length=MAX_SEQ_LENGTH,\n",
                "                dtype=None,\n",
                "                load_in_4bit=True,\n",
                "            )\n",
                "            \n",
                "            model = FastLanguageModel.get_peft_model(\n",
                "                model,\n",
                "                r=16,\n",
                "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                               \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "                lora_alpha=16,\n",
                "                lora_dropout=0.05,\n",
                "                bias=\"none\",\n",
                "                use_gradient_checkpointing=\"unsloth\",\n",
                "                random_state=42,\n",
                "            )\n",
                "            print(\"[load_model_for_dpo] Output: Loaded with Unsloth + LoRA\")\n",
                "            print(f\"========== END: load_model_for_dpo ==========\")\n",
                "            return model, tokenizer\n",
                "        except Exception as e:\n",
                "            print(f\"[load_model_for_dpo] Unsloth failed: {e}\")\n",
                "    \n",
                "    try:\n",
                "        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "        from transformers import BitsAndBytesConfig\n",
                "        \n",
                "        bnb_config = BitsAndBytesConfig(\n",
                "            load_in_4bit=True,\n",
                "            bnb_4bit_quant_type=\"nf4\",\n",
                "            bnb_4bit_compute_dtype=torch.float16,\n",
                "            bnb_4bit_use_double_quant=True,\n",
                "        )\n",
                "        \n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
                "        model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_path,\n",
                "            quantization_config=bnb_config,\n",
                "            device_map=\"auto\",\n",
                "            trust_remote_code=True,\n",
                "        )\n",
                "        \n",
                "        model = prepare_model_for_kbit_training(model)\n",
                "        \n",
                "        lora_config = LoraConfig(\n",
                "            r=16,\n",
                "            lora_alpha=16,\n",
                "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
                "            lora_dropout=0.05,\n",
                "            bias=\"none\",\n",
                "            task_type=\"CAUSAL_LM\",\n",
                "        )\n",
                "        model = get_peft_model(model, lora_config)\n",
                "        \n",
                "        print(\"[load_model_for_dpo] Output: Loaded with transformers + PEFT\")\n",
                "        print(f\"========== END: load_model_for_dpo ==========\")\n",
                "        return model, tokenizer\n",
                "        \n",
                "    except Exception as e:\n",
                "        raise RuntimeError(f\"[load_model_for_dpo] Failed to load model: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dpo_dataset = load_dpo_data(DPO_DATA_PATH)\n",
                "\n",
                "if dpo_dataset is None:\n",
                "    print(\"Cannot proceed without DPO data\")\n",
                "else:\n",
                "    print(f\"Dataset size: {len(dpo_dataset)}\")\n",
                "    print(f\"Sample: {dpo_dataset[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model, tokenizer = load_model_for_dpo(SFT_MODEL_PATH)\n",
                "\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    print(\"[Tokenizer] Set pad_token to eos_token\")\n",
                "\n",
                "if tokenizer.padding_side != \"left\":\n",
                "    tokenizer.padding_side = \"left\"\n",
                "    print(\"[Tokenizer] Set padding_side to left\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    print(\"[Config] Creating DPO Config...\")\n",
                "    dpo_config = DPOConfig(\n",
                "        output_dir=OUTPUT_DIR,\n",
                "        beta=BETA,\n",
                "        learning_rate=LEARNING_RATE,\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
                "        num_train_epochs=NUM_EPOCHS,\n",
                "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
                "        max_length=MAX_LENGTH,\n",
                "        warmup_ratio=0.1,\n",
                "        lr_scheduler_type=\"cosine\",\n",
                "        optim=\"adamw_8bit\" if UNSLOTH_AVAILABLE else \"adamw_torch\",\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        logging_steps=10,\n",
                "        save_strategy=\"epoch\",\n",
                "        report_to=\"none\",\n",
                "        remove_unused_columns=False,\n",
                "        seed=42,\n",
                "    )\n",
                "    print(\"[Config] DPO config created successfully\")\n",
                "except TypeError as e:\n",
                "    print(f\"[Config] DPOConfig error: {e}\")\n",
                "    print(\"[Config] Using minimal config\")\n",
                "    from transformers import TrainingArguments\n",
                "    dpo_config = TrainingArguments(\n",
                "        output_dir=OUTPUT_DIR,\n",
                "        learning_rate=LEARNING_RATE,\n",
                "        per_device_train_batch_size=BATCH_SIZE,\n",
                "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
                "        num_train_epochs=NUM_EPOCHS,\n",
                "        warmup_ratio=0.1,\n",
                "        fp16=True,\n",
                "        logging_steps=10,\n",
                "        save_strategy=\"epoch\",\n",
                "        report_to=\"none\",\n",
                "        seed=42,\n",
                "    )\n",
                "    BETA = 0.1\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    print(\"[Trainer] Initializing DPOTrainer...\")\n",
                "    trainer = DPOTrainer(\n",
                "        model=model,\n",
                "        ref_model=None,\n",
                "        args=dpo_config,\n",
                "        train_dataset=dpo_dataset,\n",
                "        tokenizer=tokenizer,\n",
                "        beta=BETA if 'beta' not in str(dpo_config) else None,\n",
                "    )\n",
                "    print(\"[Trainer] DPOTrainer created successfully\")\n",
                "except TypeError as e:\n",
                "    print(f\"[Trainer] Init error: {e}\")\n",
                "    print(\"[Trainer] Trying alternative initialization...\")\n",
                "    trainer = DPOTrainer(\n",
                "        model=model,\n",
                "        args=dpo_config,\n",
                "        train_dataset=dpo_dataset,\n",
                "        tokenizer=tokenizer,\n",
                "    )\n",
                "    print(\"[Trainer] DPOTrainer created with fallback\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"========== START: Training ==========\")\n",
                "print(f\"[Training] Dataset size: {len(dpo_dataset)}\")\n",
                "print(f\"[Training] Batch size: {BATCH_SIZE}\")\n",
                "print(f\"[Training] Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
                "print(f\"[Training] Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
                "print(f\"[Training] Epochs: {NUM_EPOCHS}\")\n",
                "print(f\"[Training] Beta: {BETA}\")\n",
                "\n",
                "try:\n",
                "    trainer.train()\n",
                "    print(\"[Training] Training completed successfully\")\n",
                "    print(\"========== END: Training ==========\")\n",
                "except RuntimeError as e:\n",
                "    if \"out of memory\" in str(e).lower():\n",
                "        print(f\"[Training] GPU OOM error: {e}\")\n",
                "        print(\"[Training] Try reducing BATCH_SIZE or MAX_LENGTH\")\n",
                "        torch.cuda.empty_cache()\n",
                "    else:\n",
                "        raise e\n",
                "except Exception as e:\n",
                "    print(f\"[Training] Error: {e}\")\n",
                "    raise e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"========== START: Saving Model ==========\")\n",
                "try:\n",
                "    os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
                "    \n",
                "    if UNSLOTH_AVAILABLE:\n",
                "        model.save_pretrained(FINAL_MODEL_DIR)\n",
                "        tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
                "    else:\n",
                "        trainer.save_model(FINAL_MODEL_DIR)\n",
                "        tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
                "    \n",
                "    print(f\"[Save] Output: Model saved to {FINAL_MODEL_DIR}\")\n",
                "    \n",
                "    files = os.listdir(FINAL_MODEL_DIR)\n",
                "    print(f\"[Save] Saved files: {files}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"[Save] Error saving model: {e}\")\n",
                "    backup_dir = \"/content/dpo_model_backup\"\n",
                "    try:\n",
                "        os.makedirs(backup_dir, exist_ok=True)\n",
                "        trainer.save_model(backup_dir)\n",
                "        tokenizer.save_pretrained(backup_dir)\n",
                "        print(f\"[Save] Saved backup to {backup_dir}\")\n",
                "    except Exception as e2:\n",
                "        print(f\"[Save] Backup also failed: {e2}\")\n",
                "print(\"========== END: Saving Model ==========\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n========== START: Testing ==========\")\n",
                "\n",
                "test_prompts = [\n",
                "    \"import pandas \",\n",
                "    \"import numpy \",\n",
                "    \"def fibonacci(n):\\n    \",\n",
                "    \"class DataProcessor:\\n    def __init__(self):\\n        \",\n",
                "]\n",
                "\n",
                "model.eval()\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    try:\n",
                "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
                "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=32,\n",
                "                temperature=0.7,\n",
                "                do_sample=True,\n",
                "                pad_token_id=tokenizer.eos_token_id,\n",
                "            )\n",
                "        \n",
                "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        completion = generated[len(prompt):]\n",
                "        \n",
                "        print(f\"[Test] Prompt: {prompt.strip()}\")\n",
                "        print(f\"[Test] Completion: {completion.strip()[:50]}\")\n",
                "        print(\"-\" * 50)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"[Test] Error testing prompt '{prompt[:20]}...': {e}\")\n",
                "print(\"========== END: Testing ==========\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if UNSLOTH_AVAILABLE:\n",
                "    try:\n",
                "        merged_dir = \"./dpo_merged_model\"\n",
                "        print(f\"========== START: Merging ==========\")\n",
                "        model.save_pretrained_merged(\n",
                "            merged_dir,\n",
                "            tokenizer,\n",
                "            save_method=\"merged_16bit\",\n",
                "        )\n",
                "        print(f\"[Merge] Output: Merged model saved to {merged_dir}\")\n",
                "        print(f\"========== END: Merging ==========\")\n",
                "    except Exception as e:\n",
                "        print(f\"[Merge] Failed: {e}\")\n",
                "else:\n",
                "    try:\n",
                "        from peft import PeftModel\n",
                "        merged_dir = \"./dpo_merged_model\"\n",
                "        print(f\"========== START: Merging ==========\")\n",
                "        merged_model = model.merge_and_unload()\n",
                "        merged_model.save_pretrained(merged_dir)\n",
                "        tokenizer.save_pretrained(merged_dir)\n",
                "        print(f\"[Merge] Output: Merged model saved to {merged_dir}\")\n",
                "        print(f\"========== END: Merging ==========\")\n",
                "    except Exception as e:\n",
                "        print(f\"[Merge] Failed: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"DPO TRAINING COMPLETE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"LoRA model: {FINAL_MODEL_DIR}\")\n",
                "if os.path.exists(\"./dpo_merged_model\"):\n",
                "    print(f\"Merged model: ./dpo_merged_model\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Evaluate DPO model vs SFT model\")\n",
                "print(\"2. Convert to GGUF for deployment\")\n",
                "print(\"3. Deploy with server_gguf.py\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
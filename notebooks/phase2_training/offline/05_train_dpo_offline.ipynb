{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DPO Training (Offline Version)\n",
                "\n",
                "Train model using Direct Preference Optimization with self-play preference pairs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import torch\n",
                "import numpy as np\n",
                "\n",
                "print(f\"NumPy: {np.__version__}\")\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SFT_MODEL_PATH = './final_model'\n",
                "DPO_DATA_PATH = './dpo_preference_data.jsonl'\n",
                "OUTPUT_DIR = './dpo_outputs'\n",
                "FINAL_MODEL_DIR = './dpo_final_model'\n",
                "\n",
                "MAX_SEQ_LENGTH = 2048\n",
                "LEARNING_RATE = 5e-5\n",
                "BATCH_SIZE = 2\n",
                "GRADIENT_ACCUMULATION = 4\n",
                "NUM_EPOCHS = 1\n",
                "BETA = 0.1\n",
                "\n",
                "if not os.path.exists(SFT_MODEL_PATH):\n",
                "    SFT_MODEL_PATH = '/app/models/Qwen2.5-Coder-0.5B-Instruct'\n",
                "    print(f\"Using base model: {SFT_MODEL_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "from trl import DPOTrainer, DPOConfig\n",
                "from datasets import Dataset\n",
                "\n",
                "print(f\"Loading model from {SFT_MODEL_PATH}...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=SFT_MODEL_PATH,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dtype=None,\n",
                "    load_in_4bit=True,\n",
                ")\n",
                "\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                "    random_state=42,\n",
                ")\n",
                "print(\"✓ Model loaded with LoRA!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load DPO data\n",
                "data = []\n",
                "with open(DPO_DATA_PATH, 'r', encoding='utf-8') as f:\n",
                "    for line in f:\n",
                "        item = json.loads(line.strip())\n",
                "        if 'prompt' in item and 'chosen' in item and 'rejected' in item:\n",
                "            data.append({\n",
                "                'prompt': str(item['prompt']),\n",
                "                'chosen': str(item['chosen']),\n",
                "                'rejected': str(item['rejected']),\n",
                "            })\n",
                "\n",
                "dpo_dataset = Dataset.from_list(data)\n",
                "print(f\"Loaded {len(dpo_dataset)} preference pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"left\"\n",
                "\n",
                "dpo_config = DPOConfig(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    beta=BETA,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    max_prompt_length=512,\n",
                "    max_length=1024,\n",
                "    warmup_ratio=0.1,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    optim=\"adamw_8bit\",\n",
                "    fp16=not torch.cuda.is_bf16_supported(),\n",
                "    bf16=torch.cuda.is_bf16_supported(),\n",
                "    logging_steps=10,\n",
                "    save_strategy=\"epoch\",\n",
                "    report_to=\"none\",\n",
                "    remove_unused_columns=False,\n",
                "    seed=42,\n",
                ")\n",
                "\n",
                "trainer = DPOTrainer(\n",
                "    model=model,\n",
                "    ref_model=None,\n",
                "    args=dpo_config,\n",
                "    train_dataset=dpo_dataset,\n",
                "    tokenizer=tokenizer,\n",
                ")\n",
                "print(\"✓ DPOTrainer initialized!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting DPO training...\")\n",
                "trainer.train()\n",
                "print(\"✓ Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
                "model.save_pretrained(FINAL_MODEL_DIR)\n",
                "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
                "print(f\"✓ Model saved to {FINAL_MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge LoRA weights\n",
                "merged_dir = './dpo_merged_model'\n",
                "model.save_pretrained_merged(\n",
                "    merged_dir,\n",
                "    tokenizer,\n",
                "    save_method=\"merged_16bit\",\n",
                ")\n",
                "print(f\"✓ Merged model saved to {merged_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"DPO TRAINING COMPLETE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"LoRA model: {FINAL_MODEL_DIR}\")\n",
                "print(f\"Merged model: ./dpo_merged_model\")\n",
                "print(\"\\nNext: Convert to GGUF for deployment\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

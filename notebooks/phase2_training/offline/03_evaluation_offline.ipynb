{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2 - Model Evaluation (Offline Version)\n",
                "\n",
                "Evaluate trained model and test code completion quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import numpy as np\n",
                "\n",
                "# === CONFIG ===\n",
                "MODEL_PATH = './final_model'\n",
                "TEST_PATH = './split_data/test.jsonl'\n",
                "\n",
                "print(f\"NumPy: {np.__version__}\")\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install evaluation dependencies if needed\n",
                "try:\n",
                "    import Levenshtein\n",
                "except ImportError:\n",
                "    import subprocess\n",
                "    subprocess.check_call(['pip', 'install', 'python-Levenshtein'])\n",
                "    import Levenshtein"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "\n",
                "print(f\"Loading model from {MODEL_PATH}...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_PATH,\n",
                "    max_seq_length=1024,\n",
                "    dtype=None,\n",
                "    load_in_4bit=True,\n",
                ")\n",
                "FastLanguageModel.for_inference(model)\n",
                "print(\"✓ Model loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "test_dataset = load_dataset('json', data_files=TEST_PATH, split='train')\n",
                "print(f\"Test samples: {len(test_dataset):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from difflib import SequenceMatcher\n",
                "\n",
                "def exact_match(prediction, reference):\n",
                "    return 1.0 if prediction.strip() == reference.strip() else 0.0\n",
                "\n",
                "def edit_similarity(prediction, reference):\n",
                "    if not prediction and not reference:\n",
                "        return 1.0\n",
                "    if not prediction or not reference:\n",
                "        return 0.0\n",
                "    distance = Levenshtein.distance(prediction, reference)\n",
                "    max_len = max(len(prediction), len(reference))\n",
                "    return 1.0 - (distance / max_len)\n",
                "\n",
                "def perfect_lines(prediction, reference):\n",
                "    pred_lines = prediction.strip().split('\\n')\n",
                "    ref_lines = reference.strip().split('\\n')\n",
                "    if not ref_lines:\n",
                "        return 0.0\n",
                "    matches = sum(1 for p, r in zip(pred_lines, ref_lines) if p.strip() == r.strip())\n",
                "    return matches / len(ref_lines)\n",
                "\n",
                "def matched_ratio(prediction, reference):\n",
                "    return SequenceMatcher(None, prediction, reference).ratio()\n",
                "\n",
                "print(\"✓ Metrics loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Evaluate on subset\n",
                "test_subset = test_dataset.shuffle(seed=42).select(range(min(500, len(test_dataset))))\n",
                "print(f\"Evaluating on {len(test_subset)} samples\")\n",
                "\n",
                "results = {'em': [], 'es': [], 'pl': [], 'mr': []}\n",
                "\n",
                "def parse_fim(text):\n",
                "    prefix = text.split('<PRE>')[1].split('<SUF>')[0].strip() if '<PRE>' in text else \"\"\n",
                "    suffix = text.split('<SUF>')[1].split('<MID>')[0].strip() if '<SUF>' in text else \"\"\n",
                "    middle = text.split('<MID>')[1].split('<')[0].strip() if '<MID>' in text else \"\"\n",
                "    return prefix, suffix, middle\n",
                "\n",
                "def generate_completion(prefix, suffix):\n",
                "    prompt = f\"<PRE> {prefix} <SUF> {suffix} <MID>\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True).to(\"cuda\")\n",
                "    outputs = model.generate(**inputs, max_new_tokens=32, temperature=0.2, pad_token_id=tokenizer.eos_token_id)\n",
                "    result = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
                "    return result.split('<MID>')[1].split('<')[0].strip() if '<MID>' in result else \"\"\n",
                "\n",
                "for sample in tqdm(test_subset, desc=\"Evaluating\"):\n",
                "    prefix, suffix, reference = parse_fim(sample['text'])\n",
                "    prediction = generate_completion(prefix, suffix)\n",
                "    \n",
                "    results['em'].append(exact_match(prediction, reference))\n",
                "    results['es'].append(edit_similarity(prediction, reference))\n",
                "    results['pl'].append(perfect_lines(prediction, reference))\n",
                "    results['mr'].append(matched_ratio(prediction, reference))\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(\"EVALUATION RESULTS:\")\n",
                "print(f\"{'='*50}\")\n",
                "print(f\"Exact Match (EM):      {sum(results['em'])/len(results['em'])*100:.2f}%\")\n",
                "print(f\"Edit Similarity (ES):  {sum(results['es'])/len(results['es'])*100:.2f}%\")\n",
                "print(f\"Perfect Lines (PL):    {sum(results['pl'])/len(results['pl'])*100:.2f}%\")\n",
                "print(f\"Matched Ratio (MR):    {sum(results['mr'])/len(results['mr'])*100:.2f}%\")\n",
                "print(f\"{'='*50}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick test\n",
                "def complete(prefix, suffix=\"\"):\n",
                "    prompt = f\"<PRE> {prefix} <SUF> {suffix} <MID>\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True).to(\"cuda\")\n",
                "    outputs = model.generate(**inputs, max_new_tokens=32, temperature=0.2, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
                "    result = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
                "    if \"<MID>\" in result:\n",
                "        return result.split(\"<MID>\")[1].split(\"<\")[0].strip()\n",
                "    return result\n",
                "\n",
                "print(\"Python test:\")\n",
                "print(complete(\"def calculate_sum(a, b):\\n    \"))\n",
                "print(\"\\nJava test:\")\n",
                "print(complete(\"public class User {\\n    private String \"))\n",
                "print(\"\\nC++ test:\")\n",
                "print(complete(\"#include <iostream>\\nint main() {\\n    \"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
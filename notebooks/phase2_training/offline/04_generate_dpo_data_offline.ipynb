{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DPO Data Generation (Offline Version)\n",
                "\n",
                "Generate preference pairs for DPO training using self-play generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import torch\n",
                "import random\n",
                "import ast\n",
                "import re\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "print(f\"NumPy: {np.__version__}\")\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_PATH = './final_model'\n",
                "FIM_DATA_PATH = './fim_dataset.jsonl'\n",
                "OUTPUT_PATH = './dpo_preference_data.jsonl'\n",
                "NUM_SAMPLES = 5000\n",
                "NUM_GENERATIONS = 5\n",
                "MAX_NEW_TOKENS = 64\n",
                "TEMPERATURE = 0.8\n",
                "\n",
                "if not os.path.exists(MODEL_PATH):\n",
                "    MODEL_PATH = '/app/models/Qwen2.5-Coder-0.5B-Instruct'\n",
                "    print(f\"Using base model: {MODEL_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "\n",
                "print(f\"Loading model from {MODEL_PATH}...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_PATH,\n",
                "    max_seq_length=2048,\n",
                "    dtype=None,\n",
                "    load_in_4bit=True,\n",
                ")\n",
                "FastLanguageModel.for_inference(model)\n",
                "print(\"✓ Model loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_language(text):\n",
                "    if not text:\n",
                "        return \"unknown\"\n",
                "    if \"public class\" in text or \"System.out\" in text:\n",
                "        return \"java\"\n",
                "    if \"#include\" in text or \"std::\" in text:\n",
                "        return \"cpp\"\n",
                "    return \"python\"\n",
                "\n",
                "def check_syntax_partial(completion, language):\n",
                "    if not completion or not completion.strip():\n",
                "        return False\n",
                "    return len(completion.strip()) > 0\n",
                "\n",
                "def check_length(completion):\n",
                "    length = len(completion.split())\n",
                "    if length < 1:\n",
                "        return 0.0\n",
                "    if length > 100:\n",
                "        return 0.3\n",
                "    return 1.0\n",
                "\n",
                "def score_completion(prompt, completion, language):\n",
                "    scores = {}\n",
                "    scores[\"syntax\"] = 1.0 if check_syntax_partial(completion, language) else 0.0\n",
                "    scores[\"length\"] = check_length(completion)\n",
                "    weights = {\"syntax\": 0.5, \"length\": 0.5}\n",
                "    final_score = sum(scores[k] * weights[k] for k in scores)\n",
                "    return final_score * 100, scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_completions(prompt, num_generations=5):\n",
                "    completions = []\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
                "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "    \n",
                "    for i in range(num_generations):\n",
                "        with torch.no_grad():\n",
                "            outputs = model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=MAX_NEW_TOKENS,\n",
                "                temperature=TEMPERATURE,\n",
                "                do_sample=True,\n",
                "                top_p=0.95,\n",
                "                pad_token_id=tokenizer.eos_token_id,\n",
                "            )\n",
                "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        completion = generated[len(prompt):].strip()\n",
                "        if completion:\n",
                "            completions.append(completion)\n",
                "    return completions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "samples = []\n",
                "with open(FIM_DATA_PATH, 'r', encoding='utf-8') as f:\n",
                "    for i, line in enumerate(f):\n",
                "        if i >= NUM_SAMPLES * 2:\n",
                "            break\n",
                "        try:\n",
                "            data = json.loads(line.strip())\n",
                "            if 'text' in data and '<fim_middle>' in data['text']:\n",
                "                samples.append(data)\n",
                "        except:\n",
                "            continue\n",
                "\n",
                "random.shuffle(samples)\n",
                "samples = samples[:NUM_SAMPLES]\n",
                "print(f\"Loaded {len(samples)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "preference_data = []\n",
                "\n",
                "for sample in tqdm(samples, desc=\"Generating preference pairs\"):\n",
                "    fim_text = sample.get('text', '')\n",
                "    if '<fim_middle>' in fim_text:\n",
                "        prompt = fim_text.split('<fim_middle>')[0] + '<fim_middle>'\n",
                "    else:\n",
                "        continue\n",
                "    \n",
                "    if len(prompt) < 10:\n",
                "        continue\n",
                "    \n",
                "    language = detect_language(prompt)\n",
                "    completions = generate_completions(prompt, NUM_GENERATIONS)\n",
                "    \n",
                "    if len(completions) < 2:\n",
                "        continue\n",
                "    \n",
                "    scored = [(c, score_completion(prompt, c, language)[0]) for c in completions]\n",
                "    scored.sort(key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    chosen, chosen_score = scored[0]\n",
                "    rejected, rejected_score = scored[-1]\n",
                "    \n",
                "    if chosen != rejected and abs(chosen_score - rejected_score) >= 5:\n",
                "        preference_data.append({\n",
                "            \"prompt\": prompt,\n",
                "            \"chosen\": chosen,\n",
                "            \"rejected\": rejected,\n",
                "            \"chosen_score\": chosen_score,\n",
                "            \"rejected_score\": rejected_score,\n",
                "            \"language\": language,\n",
                "        })\n",
                "\n",
                "print(f\"Generated {len(preference_data)} preference pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
                "    for item in preference_data:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "print(f\"✓ Saved to {OUTPUT_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

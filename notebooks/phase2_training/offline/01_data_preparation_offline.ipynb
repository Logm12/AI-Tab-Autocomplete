{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2 - Data Preparation (Offline Version)\n",
        "\n",
        "**Objective**: Load FIM `.jsonl` dataset and split by repository into train/val/test sets.\n",
        "\n",
        "**Environment**: Offline GPU machine with Jupyter Lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# === CONFIG: Update paths as needed ===\n",
        "FIM_DATA_PATH = './fim_dataset.jsonl'\n",
        "OUTPUT_DIR = './split_data'\n",
        "\n",
        "print(f\"FIM Dataset: {FIM_DATA_PATH}\")\n",
        "print(f\"Output Dir: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl(file_path):\n",
        "    samples = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                samples.append(data)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Warning: Skipping invalid JSON at line {line_num}: {e}\")\n",
        "    return samples\n",
        "\n",
        "def validate_fim_format(sample):\n",
        "    text = sample.get('text', '')\n",
        "    return '<PRE>' in text and '<SUF>' in text and '<MID>' in text\n",
        "\n",
        "print(f\"Loading {FIM_DATA_PATH}...\")\n",
        "samples = load_jsonl(FIM_DATA_PATH)\n",
        "valid_samples = [s for s in samples if validate_fim_format(s)]\n",
        "print(f\"Total samples: {len(samples):,}\")\n",
        "print(f\"Valid FIM samples: {len(valid_samples):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_repository(sample):\n",
        "    if 'repository' in sample:\n",
        "        return sample['repository']\n",
        "    if 'file_path' in sample:\n",
        "        path = sample['file_path']\n",
        "        parts = path.split('/')\n",
        "        if len(parts) >= 3:\n",
        "            return f\"{parts[1]}/{parts[2]}\"\n",
        "    text_sample = sample.get('text', '')[:100]\n",
        "    return f\"unknown_{hash(text_sample) % 10000}\"\n",
        "\n",
        "repo_to_samples = defaultdict(list)\n",
        "for sample in valid_samples:\n",
        "    repo = extract_repository(sample)\n",
        "    repo_to_samples[repo].append(sample)\n",
        "\n",
        "print(f\"Total repositories: {len(repo_to_samples):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "all_repos = list(repo_to_samples.keys())\n",
        "random.shuffle(all_repos)\n",
        "\n",
        "n_repos = len(all_repos)\n",
        "train_end = int(n_repos * 0.80)\n",
        "val_end = int(n_repos * 0.85)\n",
        "\n",
        "train_repos = all_repos[:train_end]\n",
        "val_repos = all_repos[train_end:val_end]\n",
        "test_repos = all_repos[val_end:]\n",
        "\n",
        "train_samples = [s for repo in train_repos for s in repo_to_samples[repo]]\n",
        "val_samples = [s for repo in val_repos for s in repo_to_samples[repo]]\n",
        "test_samples = [s for repo in test_repos for s in repo_to_samples[repo]]\n",
        "\n",
        "print(f\"Train: {len(train_samples):,} samples ({len(train_repos)} repos)\")\n",
        "print(f\"Val: {len(val_samples):,} samples ({len(val_repos)} repos)\")\n",
        "print(f\"Test: {len(test_samples):,} samples ({len(test_repos)} repos)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify no data leakage\n",
        "train_set = set(train_repos)\n",
        "val_set = set(val_repos)\n",
        "test_set = set(test_repos)\n",
        "\n",
        "assert len(train_set & val_set) == 0, \"Leakage: Train/Val\"\n",
        "assert len(train_set & test_set) == 0, \"Leakage: Train/Test\"\n",
        "assert len(val_set & test_set) == 0, \"Leakage: Val/Test\"\n",
        "print(\"✅ No data leakage detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_jsonl(samples, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in samples:\n",
        "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
        "    print(f\"✓ Saved {len(samples):,} samples to {file_path}\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "save_jsonl(train_samples, f'{OUTPUT_DIR}/train.jsonl')\n",
        "save_jsonl(val_samples, f'{OUTPUT_DIR}/val.jsonl')\n",
        "save_jsonl(test_samples, f'{OUTPUT_DIR}/test.jsonl')\n",
        "\n",
        "print(\"\\n✅ Data preparation complete!\")\n",
        "print(\"Next: Run 02_training_offline.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

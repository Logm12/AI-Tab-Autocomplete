{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DPO Data Generation - Self-Play & Auto-Scoring\n",
                "\n",
                "Generate preference pairs (chosen/rejected) for DPO training using self-play generation and auto-scoring."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import sys\n",
                "\n",
                "def install_package(package):\n",
                "    try:\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
                "        print(f\"[install_package] Successfully installed {package}\")\n",
                "        return True\n",
                "    except Exception as e:\n",
                "        print(f\"[install_package] Failed to install {package}: {e}\")\n",
                "        return False\n",
                "\n",
                "required_packages = [\n",
                "    \"unsloth\",\n",
                "    \"transformers>=4.36.0\",\n",
                "    \"datasets\",\n",
                "    \"torch\",\n",
                "    \"trl>=0.7.0\",\n",
                "    \"accelerate\",\n",
                "    \"bitsandbytes\"\n",
                "]\n",
                "\n",
                "print(\"========== START: Installing Packages ==========\")\n",
                "for pkg in required_packages:\n",
                "    install_package(pkg)\n",
                "print(\"========== END: Installing Packages ==========\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import torch\n",
                "import random\n",
                "import ast\n",
                "import re\n",
                "from pathlib import Path\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "try:\n",
                "    from unsloth import FastLanguageModel\n",
                "except ImportError:\n",
                "    print(\"Unsloth not available, using transformers\")\n",
                "    FastLanguageModel = None\n",
                "\n",
                "try:\n",
                "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"transformers required: {e}\")\n",
                "\n",
                "try:\n",
                "    from datasets import load_dataset\n",
                "except ImportError as e:\n",
                "    raise ImportError(f\"datasets required: {e}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"WARNING: No GPU detected. Generation will be slow.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_PATH = \"./final_model\"\n",
                "FIM_DATA_PATH = \"fim_dataset.jsonl\"\n",
                "OUTPUT_PATH = \"dpo_preference_data.jsonl\"\n",
                "NUM_SAMPLES = 5000\n",
                "NUM_GENERATIONS = 5\n",
                "MAX_NEW_TOKENS = 64\n",
                "TEMPERATURE = 0.8\n",
                "\n",
                "if not os.path.exists(MODEL_PATH):\n",
                "    print(f\"Model not found at {MODEL_PATH}\")\n",
                "    MODEL_PATH = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\"\n",
                "    print(f\"Using base model: {MODEL_PATH}\")\n",
                "\n",
                "if not os.path.exists(FIM_DATA_PATH):\n",
                "    alt_paths = [\n",
                "        \"../phase1_data_engineering/fim_dataset.jsonl\",\n",
                "        \"/content/fim_dataset.jsonl\",\n",
                "        \"./train.jsonl\"\n",
                "    ]\n",
                "    for p in alt_paths:\n",
                "        if os.path.exists(p):\n",
                "            FIM_DATA_PATH = p\n",
                "            print(f\"Found data at: {FIM_DATA_PATH}\")\n",
                "            break\n",
                "    else:\n",
                "        print(f\"WARNING: No FIM data found. Please upload fim_dataset.jsonl\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PYTHON_CONVENTIONS = {\n",
                "    \"import pandas\": [\"as pd\"],\n",
                "    \"import numpy\": [\"as np\"],\n",
                "    \"import matplotlib.pyplot\": [\"as plt\"],\n",
                "    \"import tensorflow\": [\"as tf\"],\n",
                "    \"import seaborn\": [\"as sns\"],\n",
                "    \"import sklearn\": [],\n",
                "    \"from typing import\": [\"List\", \"Dict\", \"Optional\", \"Tuple\", \"Union\", \"Any\"],\n",
                "    \"import torch\": [],\n",
                "    \"import torch.nn\": [\"as nn\"],\n",
                "    \"import torch.nn.functional\": [\"as F\"],\n",
                "    \"from collections import\": [\"defaultdict\", \"Counter\", \"OrderedDict\"],\n",
                "    \"import cv2\": [],\n",
                "    \"from PIL import\": [\"Image\"],\n",
                "    \"import requests\": [],\n",
                "    \"import json\": [],\n",
                "    \"import os\": [],\n",
                "    \"import sys\": [],\n",
                "    \"import re\": [],\n",
                "}\n",
                "\n",
                "JAVA_CONVENTIONS = {\n",
                "    \"public static void\": [\"main(String[] args)\", \"main(String args[])\"],\n",
                "    \"System.out.\": [\"println\", \"print\", \"printf\"],\n",
                "    \"public class\": [],\n",
                "    \"private \": [],\n",
                "    \"@Override\": [],\n",
                "    \"import java.util.\": [\"List\", \"ArrayList\", \"Map\", \"HashMap\", \"Set\"],\n",
                "}\n",
                "\n",
                "CPP_CONVENTIONS = {\n",
                "    \"#include <iostream>\": [],\n",
                "    \"#include <vector>\": [],\n",
                "    \"#include <string>\": [],\n",
                "    \"using namespace\": [\"std\"],\n",
                "    \"std::\": [\"cout\", \"cin\", \"endl\", \"vector\", \"string\", \"map\"],\n",
                "    \"int main\": [\"()\", \"(int argc, char* argv[])\"],\n",
                "}\n",
                "\n",
                "IRRELEVANT_IMPORTS = {\n",
                "    \"python\": [\"react\", \"vue\", \"angular\", \"express\", \"node\", \"jquery\", \"lodash\"],\n",
                "    \"java\": [\"numpy\", \"pandas\", \"tensorflow\", \"react\", \"import os\"],\n",
                "    \"cpp\": [\"numpy\", \"pandas\", \"import \", \"from \", \"react\"],\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_language(text):\n",
                "    if not text:\n",
                "        return \"unknown\"\n",
                "    \n",
                "    text_lower = text.lower()\n",
                "    \n",
                "    if \"import \" in text or \"def \" in text or \"class \" in text and \":\" in text:\n",
                "        if \"public class\" not in text and \"#include\" not in text:\n",
                "            return \"python\"\n",
                "    \n",
                "    if \"public class\" in text or \"public static void\" in text or \"System.out\" in text:\n",
                "        return \"java\"\n",
                "    \n",
                "    if \"#include\" in text or \"std::\" in text or \"int main\" in text:\n",
                "        return \"cpp\"\n",
                "    \n",
                "    return \"python\"\n",
                "\n",
                "\n",
                "def check_syntax_python(code):\n",
                "    try:\n",
                "        ast.parse(code)\n",
                "        return True\n",
                "    except:\n",
                "        return False\n",
                "\n",
                "\n",
                "def check_syntax_partial(completion, language):\n",
                "    if not completion or not completion.strip():\n",
                "        return False\n",
                "    \n",
                "    completion = completion.strip()\n",
                "    \n",
                "    if language == \"python\":\n",
                "        invalid_patterns = [\n",
                "            r'^[\\)\\]\\}]+$',\n",
                "            r'\\s{10,}',\n",
                "            r'^[^a-zA-Z0-9_\\s]+$',\n",
                "        ]\n",
                "        for pattern in invalid_patterns:\n",
                "            if re.match(pattern, completion):\n",
                "                return False\n",
                "        return True\n",
                "    \n",
                "    return len(completion) > 0\n",
                "\n",
                "\n",
                "def check_convention_match(prompt, completion, language):\n",
                "    prompt = prompt.strip()\n",
                "    completion = completion.strip()\n",
                "    \n",
                "    conventions = PYTHON_CONVENTIONS if language == \"python\" else \\\n",
                "                  JAVA_CONVENTIONS if language == \"java\" else CPP_CONVENTIONS\n",
                "    \n",
                "    for pattern, expected_list in conventions.items():\n",
                "        if pattern in prompt:\n",
                "            if not expected_list:\n",
                "                return 0.5\n",
                "            for expected in expected_list:\n",
                "                if expected.lower() in completion.lower():\n",
                "                    return 1.0\n",
                "            return 0.2\n",
                "    \n",
                "    return 0.5\n",
                "\n",
                "\n",
                "def check_hallucination(prompt, completion, language):\n",
                "    completion_lower = completion.lower()\n",
                "    irrelevant = IRRELEVANT_IMPORTS.get(language, [])\n",
                "    \n",
                "    for term in irrelevant:\n",
                "        if term in completion_lower:\n",
                "            return 0.0\n",
                "    \n",
                "    return 1.0\n",
                "\n",
                "\n",
                "def check_length(completion):\n",
                "    length = len(completion.split())\n",
                "    \n",
                "    if length < 1:\n",
                "        return 0.0\n",
                "    if length > 100:\n",
                "        return 0.3\n",
                "    if 1 <= length <= 50:\n",
                "        return 1.0\n",
                "    return 0.7\n",
                "\n",
                "\n",
                "def check_context_relevance(prompt, completion):\n",
                "    prompt_tokens = set(prompt.lower().split())\n",
                "    completion_tokens = set(completion.lower().split())\n",
                "    \n",
                "    if not completion_tokens:\n",
                "        return 0.0\n",
                "    \n",
                "    common = prompt_tokens.intersection(completion_tokens)\n",
                "    \n",
                "    relevance = min(len(common) / max(len(completion_tokens), 1), 1.0)\n",
                "    \n",
                "    return 0.3 + 0.7 * relevance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def score_completion(prompt, completion, language):\n",
                "    # print(f\"[score_completion] Input: completion_len={len(completion)}, lang={language}\") # Commented to avoid noise\n",
                "    scores = {}\n",
                "    \n",
                "    try:\n",
                "        scores[\"syntax\"] = 1.0 if check_syntax_partial(completion, language) else 0.0\n",
                "    except Exception:\n",
                "        scores[\"syntax\"] = 0.5\n",
                "    \n",
                "    try:\n",
                "        scores[\"convention\"] = check_convention_match(prompt, completion, language)\n",
                "    except Exception:\n",
                "        scores[\"convention\"] = 0.5\n",
                "    \n",
                "    try:\n",
                "        scores[\"relevance\"] = check_context_relevance(prompt, completion)\n",
                "    except Exception:\n",
                "        scores[\"relevance\"] = 0.5\n",
                "    \n",
                "    try:\n",
                "        scores[\"length\"] = check_length(completion)\n",
                "    except Exception:\n",
                "        scores[\"length\"] = 0.5\n",
                "    \n",
                "    try:\n",
                "        scores[\"hallucination\"] = check_hallucination(prompt, completion, language)\n",
                "    except Exception:\n",
                "        scores[\"hallucination\"] = 0.5\n",
                "    \n",
                "    weights = {\n",
                "        \"syntax\": 0.30,\n",
                "        \"convention\": 0.25,\n",
                "        \"relevance\": 0.25,\n",
                "        \"length\": 0.10,\n",
                "        \"hallucination\": 0.10,\n",
                "    }\n",
                "    \n",
                "    final_score = sum(scores[k] * weights[k] for k in scores)\n",
                "    \n",
                "    # print(f\"[score_completion] Output: final_score={final_score*100:.2f}\")\n",
                "    return final_score * 100, scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_model(model_path):\n",
                "    print(f\"========== START: load_model ==========\")\n",
                "    print(f\"[load_model] Input: model_path={model_path}\")\n",
                "    \n",
                "    if FastLanguageModel is not None:\n",
                "        try:\n",
                "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "                model_name=model_path,\n",
                "                max_seq_length=2048,\n",
                "                dtype=None,\n",
                "                load_in_4bit=True,\n",
                "            )\n",
                "            FastLanguageModel.for_inference(model)\n",
                "            print(\"[load_model] Output: Loaded with Unsloth\")\n",
                "            print(f\"========== END: load_model ==========\")\n",
                "            return model, tokenizer\n",
                "        except Exception as e:\n",
                "            print(f\"[load_model] Unsloth failed: {e}. Trying transformers...\")\n",
                "    \n",
                "    try:\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
                "        model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_path,\n",
                "            torch_dtype=torch.float16,\n",
                "            device_map=\"auto\",\n",
                "            trust_remote_code=True,\n",
                "        )\n",
                "        model.eval()\n",
                "        print(\"[load_model] Output: Loaded with transformers\")\n",
                "        print(f\"========== END: load_model ==========\")\n",
                "        return model, tokenizer\n",
                "    except Exception as e:\n",
                "        raise RuntimeError(f\"[load_model] Failed to load model: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_completions(model, tokenizer, prompt, num_generations=5, max_tokens=64, temperature=0.8):\n",
                "    # print(f\"[generate_completions] Input: prompt_len={len(prompt)}, num_generations={num_generations}\")\n",
                "    completions = []\n",
                "    \n",
                "    try:\n",
                "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
                "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "    except Exception as e:\n",
                "        print(f\"[generate_completions] Tokenization error: {e}\")\n",
                "        return []\n",
                "    \n",
                "    for i in range(num_generations):\n",
                "        try:\n",
                "            with torch.no_grad():\n",
                "                outputs = model.generate(\n",
                "                    **inputs,\n",
                "                    max_new_tokens=max_tokens,\n",
                "                    temperature=temperature,\n",
                "                    do_sample=True,\n",
                "                    top_p=0.95,\n",
                "                    top_k=50,\n",
                "                    pad_token_id=tokenizer.eos_token_id,\n",
                "                    num_return_sequences=1,\n",
                "                )\n",
                "            \n",
                "            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "            completion = generated[len(prompt):].strip()\n",
                "            \n",
                "            if completion:\n",
                "                completions.append(completion)\n",
                "        except Exception as e:\n",
                "            print(f\"[generate_completions] Generation error {i}: {e}\")\n",
                "            continue\n",
                "    \n",
                "    # print(f\"[generate_completions] Output: {len(completions)} completions generated\")\n",
                "    return completions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_fim_samples(data_path, num_samples):\n",
                "    print(f\"========== START: load_fim_samples ==========\")\n",
                "    print(f\"[load_fim_samples] Input: path={data_path}, num_samples={num_samples}\")\n",
                "    samples = []\n",
                "    \n",
                "    try:\n",
                "        with open(data_path, 'r', encoding='utf-8') as f:\n",
                "            for i, line in enumerate(f):\n",
                "                if i >= num_samples * 2:\n",
                "                    break\n",
                "                try:\n",
                "                    data = json.loads(line.strip())\n",
                "                    if 'text' in data and '<fim_middle>' in data['text']:\n",
                "                        samples.append(data)\n",
                "                except json.JSONDecodeError:\n",
                "                    continue\n",
                "    except FileNotFoundError:\n",
                "        print(f\"[load_fim_samples] File not found: {data_path}\")\n",
                "        return []\n",
                "    except Exception as e:\n",
                "        print(f\"[load_fim_samples] Error loading data: {e}\")\n",
                "        return []\n",
                "    \n",
                "    random.shuffle(samples)\n",
                "    print(f\"[load_fim_samples] Output: {len(samples[:num_samples])} samples loaded\")\n",
                "    print(f\"========== END: load_fim_samples ==========\")\n",
                "    return samples[:num_samples]\n",
                "\n",
                "\n",
                "def extract_prompt_from_fim(fim_text):\n",
                "    if '<fim_middle>' in fim_text:\n",
                "        parts = fim_text.split('<fim_middle>')\n",
                "        return parts[0] + '<fim_middle>'\n",
                "    return fim_text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model, tokenizer = load_model(MODEL_PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Loading {NUM_SAMPLES} samples from {FIM_DATA_PATH}...\")\n",
                "samples = load_fim_samples(FIM_DATA_PATH, NUM_SAMPLES)\n",
                "print(f\"Loaded {len(samples)} samples\")\n",
                "\n",
                "if len(samples) == 0:\n",
                "    print(\"No samples loaded. Please check data path.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "preference_data = []\n",
                "failed_count = 0\n",
                "skip_count = 0\n",
                "\n",
                "print(f\"========== START: Preference Generation Loop ==========\")\n",
                "for i, sample in tqdm(enumerate(samples), total=len(samples), desc=\"Generating preference pairs\"):\n",
                "    try:\n",
                "        fim_text = sample.get('text', '')\n",
                "        prompt = extract_prompt_from_fim(fim_text)\n",
                "        \n",
                "        if not prompt or len(prompt) < 10:\n",
                "            skip_count += 1\n",
                "            continue\n",
                "        \n",
                "        language = detect_language(prompt)\n",
                "        \n",
                "        completions = generate_completions(\n",
                "            model, tokenizer, prompt,\n",
                "            num_generations=NUM_GENERATIONS,\n",
                "            max_tokens=MAX_NEW_TOKENS,\n",
                "            temperature=TEMPERATURE\n",
                "        )\n",
                "        print(f\"[Loop] Sample {i}: Prompt='{prompt[:30]}...', Lang={language}, Generated={len(completions)}\")\n",
                "        \n",
                "        if len(completions) < 2:\n",
                "            skip_count += 1\n",
                "            continue\n",
                "        \n",
                "        scored = []\n",
                "        for comp in completions:\n",
                "            score, details = score_completion(prompt, comp, language)\n",
                "            scored.append((comp, score, details))\n",
                "        \n",
                "        scored.sort(key=lambda x: x[1], reverse=True)\n",
                "        \n",
                "        chosen = scored[0][0]\n",
                "        rejected = scored[-1][0]\n",
                "        chosen_score = scored[0][1]\n",
                "        rejected_score = scored[-1][1]\n",
                "        \n",
                "        print(f\"   Scores: Chosen={chosen_score:.1f}, Rejected={rejected_score:.1f}\")\n",
                "        \n",
                "        if chosen == rejected or abs(chosen_score - rejected_score) < 5:\n",
                "            print(\"   Skipping: Scores too close or identical\")\n",
                "            skip_count += 1\n",
                "            continue\n",
                "        \n",
                "        preference_data.append({\n",
                "            \"prompt\": prompt,\n",
                "            \"chosen\": chosen,\n",
                "            \"rejected\": rejected,\n",
                "            \"chosen_score\": chosen_score,\n",
                "            \"rejected_score\": rejected_score,\n",
                "            \"language\": language,\n",
                "        })\n",
                "        \n",
                "    except Exception as e:\n",
                "        failed_count += 1\n",
                "        if failed_count <= 5:\n",
                "            print(f\"[Loop] Error processing sample {i}: {e}\")\n",
                "        continue\n",
                "\n",
                "print(f\"========== END: Preference Generation Loop ==========\")\n",
                "print(f\"\\nGenerated {len(preference_data)} preference pairs\")\n",
                "print(f\"Skipped: {skip_count}, Failed: {failed_count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if preference_data:\n",
                "    print(\"\\n========== Sample Preference Pair ==========\")\n",
                "    sample = preference_data[0]\n",
                "    print(f\"Prompt: {sample['prompt'][:100]}...\")\n",
                "    print(f\"Chosen ({sample['chosen_score']:.1f}): {sample['chosen'][:80]}...\")\n",
                "    print(f\"Rejected ({sample['rejected_score']:.1f}): {sample['rejected'][:80]}...\")\n",
                "    print(\"==========================================\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
                "        for item in preference_data:\n",
                "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "    print(f\"[Save] Successfully saved {len(preference_data)} pairs to {OUTPUT_PATH}\")\n",
                "except Exception as e:\n",
                "    print(f\"[Save] Error: {e}\")\n",
                "    backup_path = \"/content/dpo_data_backup.jsonl\"\n",
                "    try:\n",
                "        with open(backup_path, 'w', encoding='utf-8') as f:\n",
                "            for item in preference_data:\n",
                "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "        print(f\"[Save] Saved backup to {backup_path}\")\n",
                "    except:\n",
                "        print(\"[Save] Failed to save data anywhere.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if preference_data:\n",
                "    scores = [p['chosen_score'] for p in preference_data]\n",
                "    rejected_scores = [p['rejected_score'] for p in preference_data]\n",
                "    \n",
                "    print(f\"\\n========== Statistics ==========\")\n",
                "    print(f\"Chosen scores - Mean: {sum(scores)/len(scores):.1f}, Min: {min(scores):.1f}, Max: {max(scores):.1f}\")\n",
                "    print(f\"Rejected scores - Mean: {sum(rejected_scores)/len(rejected_scores):.1f}, Min: {min(rejected_scores):.1f}, Max: {max(rejected_scores):.1f}\")\n",
                "    \n",
                "    lang_counts = {}\n",
                "    for p in preference_data:\n",
                "        lang = p.get('language', 'unknown')\n",
                "        lang_counts[lang] = lang_counts.get(lang, 0) + 1\n",
                "    print(f\"Language distribution: {lang_counts}\")\n",
                "    print(\"===============================\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb636d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import Levenshtein\n",
    "from difflib import SequenceMatcher\n",
    "MODEL_CONFIGS = {\n",
    "    \"Base Model\": {\n",
    "        \"path\": \"Qwen/Qwen2.5-Coder-0.5B-Instruct\",\n",
    "        \"type\": \"base\",\n",
    "        \"quantized\": False\n",
    "    },\n",
    "    \"SFT Merged\": {\n",
    "        \"path\": \"./final_model\",\n",
    "        \"type\": \"base\", \n",
    "        \"quantized\": False\n",
    "    },\n",
    "    \"SFT Quantized\": {\n",
    "        \"path\": \"./final_model\",\n",
    "        \"type\": \"base\",\n",
    "        \"quantized\": True\n",
    "    },\n",
    "    \"DPO Adapter\": {\n",
    "        \"path\": \"Qwen/Qwen2.5-Coder-0.5B-Instruct\", \n",
    "        \"adapter\": \"./dpo_model\",\n",
    "        \"type\": \"adapter\",\n",
    "        \"quantized\": False\n",
    "    },\n",
    "    \"DPO Merged\": {\n",
    "        \"path\": \"./dpo_merged_model\",\n",
    "        \"type\": \"base\",\n",
    "        \"quantized\": False\n",
    "    },\n",
    "    \"DPO Quantized\": {\n",
    "        \"path\": \"./dpo_merged_model\",\n",
    "        \"type\": \"base\",\n",
    "        \"quantized\": True\n",
    "    }\n",
    "}\n",
    "TEST_DATA_PATH = \"./test.jsonl\"\n",
    "OUTPUT_FILE = \"model_comparison_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b445a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(prediction, reference, start_time, end_time):\n",
    "    pred = prediction.strip()\n",
    "    ref = reference.strip()\n",
    "    \n",
    "    # TTS (Time To Start / Latency in ms)\n",
    "    tts = (end_time - start_time) * 1000\n",
    "    \n",
    "    # EM (Exact Match)\n",
    "    em = 1.0 if pred == ref else 0.0\n",
    "    \n",
    "    # ES (Edit Similarity)\n",
    "    if not pred and not ref:\n",
    "        es = 1.0\n",
    "    else:\n",
    "        dist = Levenshtein.distance(pred, ref)\n",
    "        max_len = max(len(pred), len(ref))\n",
    "        es = 1.0 - (dist / max_len) if max_len > 0 else 0.0\n",
    "        \n",
    "    # PL (Perfect Lines)\n",
    "    pred_lines = pred.split('\\n')\n",
    "    ref_lines = ref.split('\\n')\n",
    "    common = 0\n",
    "    if ref_lines:\n",
    "        common = sum(1 for p, r in zip(pred_lines, ref_lines) if p.strip() == r.strip())\n",
    "        pl = common / len(ref_lines)\n",
    "    else:\n",
    "        pl = 0.0\n",
    "        \n",
    "    # MR (Matched Ratio)\n",
    "    mr = SequenceMatcher(None, pred, ref).ratio()\n",
    "    \n",
    "    # RoCC (Ratio of Correct Characters)\n",
    "    correct_chars = sum(1 for p, r in zip(pred, ref) if p == r)\n",
    "    rocc = correct_chars / max(len(ref), 1)\n",
    "    \n",
    "    return {\n",
    "        \"EM\": em,\n",
    "        \"ES\": es,\n",
    "        \"PL\": pl,\n",
    "        \"MR\": mr,\n",
    "        \"RoCC\": rocc,\n",
    "        \"TTS\": tts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4fdc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(path, num_samples=100):\n",
    "    ds = load_dataset('json', data_files=path, split='train')\n",
    "    if len(ds) > num_samples:\n",
    "        ds = ds.select(range(num_samples))\n",
    "    \n",
    "    samples = []\n",
    "    for item in ds:\n",
    "        text = item['text']\n",
    "        if '<fim_middle>' in text:\n",
    "            parts = text.split('<fim_middle>')\n",
    "            prompt = parts[0] + '<fim_middle>'\n",
    "            reference = parts[1].split('<file_sep>')[0] if '<file_sep>' in parts[1] else parts[1]\n",
    "            samples.append((prompt, reference))\n",
    "    return samples\n",
    "test_samples = load_test_data(TEST_DATA_PATH, num_samples=200)\n",
    "print(f\"Loaded {len(test_samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(config_name, config):\n",
    "    print(f\"Loading {config_name}...\")\n",
    "    results = []\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    \n",
    "    try:\n",
    "        if config[\"quantized\"]:\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name=config[\"path\"],\n",
    "                max_seq_length=2048,\n",
    "                dtype=None,\n",
    "                load_in_4bit=True\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                config[\"path\"],\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config[\"path\"])\n",
    "            \n",
    "        if config[\"type\"] == \"adapter\":\n",
    "            model = PeftModel.from_pretrained(model, config[\"adapter\"])\n",
    "            \n",
    "        if not hasattr(model, \"generate\"):\n",
    "             FastLanguageModel.for_inference(model)\n",
    "        \n",
    "        tokenizer.padding_side = \"left\"\n",
    "        if not tokenizer.pad_token:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        print(f\"Evaluating {config_name}...\")\n",
    "        for prompt, reference in tqdm(test_samples):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=128,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            \n",
    "            gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if prompt in gen_text:\n",
    "                prediction = gen_text.replace(prompt, \"\")\n",
    "            else:\n",
    "                prediction = gen_text[len(prompt):]\n",
    "                \n",
    "            metrics = calculate_metrics(prediction, reference, start, end)\n",
    "            metrics[\"Model\"] = config_name\n",
    "            results.append(metrics)\n",
    "            \n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {config_name}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for name, config in MODEL_CONFIGS.items():\n",
    "    res = evaluate_model(name, config)\n",
    "    all_results.extend(res)\n",
    "    \n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.groupby(\"Model\").mean().reset_index()\n",
    "print(\"\\nFinal Comparison:\")\n",
    "print(summary.to_markdown(index=False))\n",
    "best_model = summary.loc[summary['ES'].idxmax()]\n",
    "print(\"\\nBest Model by Edit Similarity:\")\n",
    "print(best_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

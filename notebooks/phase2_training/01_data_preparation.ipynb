{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xBfHfpH9c6T"
      },
      "source": [
        "# Phase 2 - Data Preparation & Splitting\n",
        "\n",
        "**Objective**: Load FIM `.jsonl` dataset from Phase 1 and split by repository into train/val/test sets.\n",
        "\n",
        "**Key Points**:\n",
        "- Split by **repository**, not by file (prevents data leakage)\n",
        "- Ratio: 80% train / 5% validation / 15% test\n",
        "- Verify no repository overlap between splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZDSqbv-9c6Y"
      },
      "source": [
        "## Step 1: Upload FIM Dataset\n",
        "\n",
        "Upload .jsonl files from Phase 1 to Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnFnXxjV9c6a",
        "outputId": "a7d92b48-59e3-490e-b568-01bd156467bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Copying /content/drive/MyDrive/AI-Auto-Complete/phase1_output/fim_dataset.jsonl to local...\n",
            "✓ Copied: fim_dataset.jsonl\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = '/content/drive/MyDrive/AI-Auto-Complete/phase1_output/fim_dataset.jsonl'\n",
        "os.makedirs('/content/fim_data', exist_ok=True)\n",
        "print(f\"Copying {DRIVE_PATH} to local...\")\n",
        "if os.path.exists(DRIVE_PATH):\n",
        "    shutil.copy(DRIVE_PATH, '/content/fim_data/fim_dataset.jsonl')\n",
        "    print(f\"Copied: fim_dataset.jsonl\")\n",
        "else:\n",
        "    print(f\"ERROR: File not found at {DRIVE_PATH}\")\n",
        "    print(\"Please update DRIVE_PATH to match your file location!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-wNalZH9c6d"
      },
      "source": [
        "## Step 2: Load and Validate FIM Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-vCELfJ9c6e",
        "outputId": "e069c8ca-1ba3-46b3-9134-06e5b7ba102c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 JSONL files\n",
            "\n",
            "Loading fim_dataset.jsonl...\n",
            "  Loaded 28361 valid samples\n",
            "\n",
            "==================================================\n",
            "Total samples loaded: 28,361\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    samples = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                samples.append(data)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Warning: Skipping invalid JSON at line {line_num}: {e}\")\n",
        "    return samples\n",
        "\n",
        "def validate_fim_format(sample):\n",
        "    text = sample.get('text', '')\n",
        "    return '<PRE>' in text and '<SUF>' in text and '<MID>' in text\n",
        "\n",
        "all_samples = []\n",
        "jsonl_files = glob.glob('/content/fim_data/*.jsonl')\n",
        "\n",
        "print(f\"Found {len(jsonl_files)} JSONL files\\n\")\n",
        "\n",
        "for file_path in jsonl_files:\n",
        "    print(f\"Loading {os.path.basename(file_path)}...\")\n",
        "    samples = load_jsonl(file_path)\n",
        "\n",
        "    valid_samples = [s for s in samples if validate_fim_format(s)]\n",
        "    invalid_count = len(samples) - len(valid_samples)\n",
        "\n",
        "    if invalid_count > 0:\n",
        "        print(f\"  Warning: {invalid_count} samples with invalid FIM format\")\n",
        "\n",
        "    all_samples.extend(valid_samples)\n",
        "    print(f\"  Loaded {len(valid_samples)} valid samples\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Total samples loaded: {len(all_samples):,}\")\n",
        "print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F_GlSlw9c6f"
      },
      "source": [
        "## Step 3: Extract Repository Information\n",
        "\n",
        "Extract repository from file paths to enable repository-based splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESHPEZfl9c6g",
        "outputId": "80642d4b-6771-4cba-c5a9-e3e24ce34a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total repositories: 9,318\n",
            "\n",
            "Repository size distribution:\n",
            "  Min samples per repo: 1\n",
            "  Max samples per repo: 63\n",
            "  Average samples per repo: 3.0\n",
            "\n",
            "Top 10 largest repositories:\n",
            "  1. unknown_4994: 63 samples\n",
            "  2. unknown_2348: 49 samples\n",
            "  3. unknown_3257: 48 samples\n",
            "  4. unknown_9801: 46 samples\n",
            "  5. unknown_5840: 44 samples\n",
            "  6. unknown_6417: 41 samples\n",
            "  7. unknown_3052: 33 samples\n",
            "  8. unknown_5098: 33 samples\n",
            "  9. unknown_1098: 31 samples\n",
            "  10. unknown_8483: 30 samples\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def extract_repository(sample):\n",
        "    if 'repository' in sample:\n",
        "        return sample['repository']\n",
        "\n",
        "    if 'file_path' in sample:\n",
        "        path = sample['file_path']\n",
        "        parts = path.split('/')\n",
        "        if len(parts) >= 3:\n",
        "            return f\"{parts[1]}/{parts[2]}\"\n",
        "\n",
        "    if 'url' in sample:\n",
        "        url = sample['url']\n",
        "        match = re.search(r'github\\.com/([^/]+)/([^/]+)', url)\n",
        "        if match:\n",
        "            return f\"{match.group(1)}/{match.group(2)}\"\n",
        "\n",
        "    text_sample = sample.get('text', '')[:100]\n",
        "    return f\"unknown_{hash(text_sample) % 10000}\"\n",
        "\n",
        "repo_to_samples = defaultdict(list)\n",
        "\n",
        "for sample in all_samples:\n",
        "    repo = extract_repository(sample)\n",
        "    repo_to_samples[repo].append(sample)\n",
        "\n",
        "print(f\"Total repositories: {len(repo_to_samples):,}\")\n",
        "print(f\"\\nRepository size distribution:\")\n",
        "repo_sizes = [len(samples) for samples in repo_to_samples.values()]\n",
        "print(f\"  Min samples per repo: {min(repo_sizes)}\")\n",
        "print(f\"  Max samples per repo: {max(repo_sizes)}\")\n",
        "print(f\"  Average samples per repo: {sum(repo_sizes)/len(repo_sizes):.1f}\")\n",
        "\n",
        "print(f\"\\nTop 10 largest repositories:\")\n",
        "sorted_repos = sorted(repo_to_samples.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "for i, (repo, samples) in enumerate(sorted_repos[:10], 1):\n",
        "    print(f\"  {i}. {repo}: {len(samples)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn7SlSou9c6h"
      },
      "source": [
        "## Step 4: Split by Repository (80/5/15)\n",
        "\n",
        "**Critical**: Split repositories, not individual samples, to prevent data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iclN2_Gp9c6h",
        "outputId": "a18a867b-fa29-4c25-a87c-ce5c0e732d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository split:\n",
            "  Train: 7,454 repos (80.0%)\n",
            "  Val:   466 repos (5.0%)\n",
            "  Test:  1,398 repos (15.0%)\n",
            "\n",
            "Sample split:\n",
            "  Train: 22,707 samples (80.1%)\n",
            "  Val:   1,437 samples (5.1%)\n",
            "  Test:  4,217 samples (14.9%)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "all_repos = list(repo_to_samples.keys())\n",
        "random.shuffle(all_repos)\n",
        "\n",
        "n_repos = len(all_repos)\n",
        "train_end = int(n_repos * 0.80)\n",
        "val_end = int(n_repos * 0.85)\n",
        "\n",
        "train_repos = all_repos[:train_end]\n",
        "val_repos = all_repos[train_end:val_end]\n",
        "test_repos = all_repos[val_end:]\n",
        "\n",
        "print(f\"Repository split:\")\n",
        "print(f\"  Train: {len(train_repos):,} repos ({len(train_repos)/n_repos*100:.1f}%)\")\n",
        "print(f\"  Val:   {len(val_repos):,} repos ({len(val_repos)/n_repos*100:.1f}%)\")\n",
        "print(f\"  Test:  {len(test_repos):,} repos ({len(test_repos)/n_repos*100:.1f}%)\")\n",
        "\n",
        "train_samples = []\n",
        "val_samples = []\n",
        "test_samples = []\n",
        "\n",
        "for repo in train_repos:\n",
        "    train_samples.extend(repo_to_samples[repo])\n",
        "\n",
        "for repo in val_repos:\n",
        "    val_samples.extend(repo_to_samples[repo])\n",
        "\n",
        "for repo in test_repos:\n",
        "    test_samples.extend(repo_to_samples[repo])\n",
        "\n",
        "print(f\"\\nSample split:\")\n",
        "total_samples = len(train_samples) + len(val_samples) + len(test_samples)\n",
        "print(f\"  Train: {len(train_samples):,} samples ({len(train_samples)/total_samples*100:.1f}%)\")\n",
        "print(f\"  Val:   {len(val_samples):,} samples ({len(val_samples)/total_samples*100:.1f}%)\")\n",
        "print(f\"  Test:  {len(test_samples):,} samples ({len(test_samples)/total_samples*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqFEDoLj9c6j"
      },
      "source": [
        "## Step 5: Verify No Data Leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCs0iqSP9c6j",
        "outputId": "e4ef4927-999e-45e1-be9a-398837e0fc86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Leakage Check:\n",
            "  Train ∩ Val:  0 repos\n",
            "  Train ∩ Test: 0 repos\n",
            "  Val ∩ Test:   0 repos\n",
            "\n",
            "✅ SUCCESS: No data leakage detected!\n"
          ]
        }
      ],
      "source": [
        "train_repo_set = set(train_repos)\n",
        "val_repo_set = set(val_repos)\n",
        "test_repo_set = set(test_repos)\n",
        "\n",
        "train_val_overlap = train_repo_set & val_repo_set\n",
        "train_test_overlap = train_repo_set & test_repo_set\n",
        "val_test_overlap = val_repo_set & test_repo_set\n",
        "\n",
        "print(\"Data Leakage Check:\")\n",
        "print(f\"  Train ∩ Val:  {len(train_val_overlap)} repos\")\n",
        "print(f\"  Train ∩ Test: {len(train_test_overlap)} repos\")\n",
        "print(f\"  Val ∩ Test:   {len(val_test_overlap)} repos\")\n",
        "\n",
        "if len(train_val_overlap) == 0 and len(train_test_overlap) == 0 and len(val_test_overlap) == 0:\n",
        "    print(\"\\nSUCCESS: No data leakage detected!\")\n",
        "else:\n",
        "    print(\"\\nERROR: Data leakage detected! Check your splitting logic.\")\n",
        "    if train_val_overlap:\n",
        "        print(f\"  Overlapping repos (Train/Val): {list(train_val_overlap)[:5]}\")\n",
        "    if train_test_overlap:\n",
        "        print(f\"  Overlapping repos (Train/Test): {list(train_test_overlap)[:5]}\")\n",
        "    if val_test_overlap:\n",
        "        print(f\"  Overlapping repos (Val/Test): {list(val_test_overlap)[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLQF3aGs9c6m"
      },
      "source": [
        "## Step 6: Save Split Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY9sDe729c6n",
        "outputId": "b18be9ab-575b-48f5-e5fd-80d255563d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved 22,707 samples to /content/split_data/train.jsonl\n",
            "✓ Saved 1,437 samples to /content/split_data/val.jsonl\n",
            "✓ Saved 4,217 samples to /content/split_data/test.jsonl\n",
            "\n",
            "==================================================\n",
            "Data preparation complete!\n",
            "==================================================\n",
            "\n",
            "Next step: Use these files in 02_training.ipynb\n"
          ]
        }
      ],
      "source": [
        "def save_jsonl(samples, file_path):\n",
        "    \"\"\"Save samples to JSONL file\"\"\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in samples:\n",
        "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
        "    print(f\"Saved {len(samples):,} samples to {file_path}\")\n",
        "\n",
        "# Save datasets\n",
        "os.makedirs('/content/split_data', exist_ok=True)\n",
        "\n",
        "save_jsonl(train_samples, '/content/split_data/train.jsonl')\n",
        "save_jsonl(val_samples, '/content/split_data/val.jsonl')\n",
        "save_jsonl(test_samples, '/content/split_data/test.jsonl')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Data preparation complete!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nNext step: Use these files in 02_training.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kua4Hql69c6n"
      },
      "source": [
        "## Step 7: Preview Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV8V7ghX9c6n",
        "outputId": "117298f6-48f1-4693-96b6-8002b4d11588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Training Sample:\n",
            "================================================================================\n",
            "PREFIX:\n",
            "package com.azure.monitor.opentelemetry.exporter;\n",
            "\n",
            "\n",
            "import com.azure.data.appconfiguration.ConfigurationClientBuilder;\n",
            "import io.opentelemetry.api.trace.Tracer;\n",
            "import io.opentelemetry.context.Scope;\n",
            "...\n",
            "\n",
            "SUFFIX:\n",
            "Tracer tracer = openTelemetrySdk.getTracer(\"Sample\");\n",
            "\n",
            "         \n",
            "        ConfigurationClient client = new ConfigurationClientBuilder()\n",
            "            .connectionString(\"{app-config-connection-string}\")\n",
            " ...\n",
            "\n",
            "MIDDLE (what model should learn to fill):\n",
            ".buildAndRegisterGlobal();\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "sample = random.choice(train_samples)\n",
        "text = sample['text']\n",
        "\n",
        "print(\"Random Training Sample:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if '<PRE>' in text and '<SUF>' in text and '<MID>' in text:\n",
        "    parts = text.split('<SUF>')\n",
        "    prefix = parts[0].replace('<PRE>', '').strip()\n",
        "\n",
        "    suffix_mid = parts[1].split('<MID>')\n",
        "    suffix = suffix_mid[0].strip()\n",
        "    middle = suffix_mid[1].strip() if len(suffix_mid) > 1 else ''\n",
        "\n",
        "    print(\"PREFIX:\")\n",
        "    print(prefix[:200] + \"...\" if len(prefix) > 200 else prefix)\n",
        "    print(\"\\nSUFFIX:\")\n",
        "    print(suffix[:200] + \"...\" if len(suffix) > 200 else suffix)\n",
        "    print(\"\\nMIDDLE (what model should learn to fill):\")\n",
        "    print(middle[:200] + \"...\" if len(middle) > 200 else middle)\n",
        "else:\n",
        "    print(text[:500])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
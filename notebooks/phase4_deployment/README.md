# Phase 4: Deployment - Local Server & RAG

## üéØ M·ª•c Ti√™u

Deploy model GGUF t·ª´ Phase 3 th√†nh HTTP server v·ªõi RAG context injection.

**Input**: `qwen-coder-code-completion-q4_k_m.gguf` (~400MB)  
**Output**: Local HTTP server t∆∞∆°ng th√≠ch OpenAI API + RAG system

## üìã Phase 4 G·ªìm Nh·ªØng G√¨?

### 1. **llama-server Setup**
- HTTP server tu√¢n th·ªß chu·∫©n OpenAI API
- KV cache reusing (slot management)
- Gi·∫£m ƒë·ªô tr·ªÖ khi g√µ code li√™n t·ª•c
- Ch·∫°y tr√™n CPU, kh√¥ng c·∫ßn GPU

### 2. **RAG Context Injection**
- **Semantic Chunking**: C·∫Øt code theo function/class
- **BM25 Search**: Keyword search c·ª±c nhanh
- **Smart Triggering**: Ch·ªâ ch·∫°y RAG khi c·∫ßn (`.`, `(`, etc.)
- Inject context v√†o prompt ƒë·ªÉ model hi·ªÉu codebase

### 3. **Extension Integration**
- API endpoint t∆∞∆°ng th√≠ch VS Code extension
- Completion streaming
- Context management
- Error handling

## üöÄ C√°ch Ch·∫°y

### Option 1: Google Colab (Khuy·∫øn Ngh·ªã)

1. **M·ªü Google Colab** (CPU runtime)
2. **Copy `phase4_deployment.py`** v√†o cell
3. **Upload model GGUF** t·ª´ Phase 3
4. **Ch·∫°y cell**
5. **Server ch·∫°y t·∫°i** `http://localhost:8080`
6. **Test v·ªõi API calls**

### Option 2: Local Machine

```bash
# 1. C√†i llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# 2. Ch·∫°y server
./llama-server \
  -m qwen-coder-code-completion-q4_k_m.gguf \
  -c 2048 \
  --port 8080 \
  --slots 4 \
  --cont-batching

# 3. Test
curl http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"prompt": "def hello():", "max_tokens": 64}'
```

## ‚è±Ô∏è Th·ªùi Gian

| B∆∞·ªõc | Th·ªùi gian | M√¥ t·∫£ |
|------|-----------|-------|
| Upload model | 2-3 min | Upload GGUF file |
| Setup llama.cpp | 5-10 min | Clone & compile |
| Start server | 10-20 sec | Load model v√†o RAM |
| Index codebase | 1-2 min | Chunking + BM25 index |
| **TOTAL** | **10-15 min** | |

## üìä K·∫øt Qu·∫£ Mong ƒê·ª£i

### Server Performance
```
Latency (cold):     100-200ms  (first request)
Latency (warm):     20-50ms    (with KV cache)
Throughput:         15-30 tok/s
RAM Usage:          800MB-1.2GB
```

### RAG Performance
```
Chunking:           ~1000 chunks/sec
BM25 Search:        <5ms per query
Context Injection:  <10ms overhead
```

## üì° API Endpoints

### 1. `/v1/completions` - Code Completion
```bash
POST http://localhost:8080/v1/completions
Content-Type: application/json

{
  "prompt": "<PRE> def calculate_sum(a, b): <SUF> <MID>",
  "max_tokens": 64,
  "temperature": 0.2,
  "top_p": 0.95,
  "stop": ["
